{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce5bf7b-4e70-4444-b71c-00c03fc87c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from plot_utils import gather_metrics\n",
    "from parse_levels import find_levels_in_configs, find_levels_in_configs_glob\n",
    "import pandas as pd\n",
    "import matplotlib.ticker as ticker\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37883825-173b-4593-8950-c54a07c2233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def load_from_jsonl(file_name: str) -> List[dict]:\n",
    "    def load_json_line(line: str, i: int, file_name: str):\n",
    "        try:\n",
    "            return json.loads(line)\n",
    "        except:\n",
    "            raise ValueError(f\"Error in line {i+1}\\n{line} of {file_name}\")\n",
    "\n",
    "    with open(file_name, \"r\", encoding=\"UTF-8\") as f:\n",
    "        data = [load_json_line(line, i, file_name) for i, line in enumerate(f)]\n",
    "    return data\n",
    "\n",
    "def save_to_jsonl(data, filename, write_mode=\"w\"):\n",
    "    with open(filename, write_mode) as file:\n",
    "        for item in data:\n",
    "            json_str = json.dumps(item)\n",
    "            file.write(json_str + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff7dba-1f88-406d-aa12-26b55cac71dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== record_4_20250502_004536_2183023-2182665-3, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 6200, 'val_loss': 3.2772, 'train_time': 1301740}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2772, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 5100, 'val_loss': 3.2751, 'train_time': 949528}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2751, reaching under the 3.28 target validation loss.'}\n",
      "agent v_11: {'val_loss': 3.2789, 'train_time': 925150}\n",
      "====== record_4_20250502_004536_2183023-2182665-3, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 6200, 'val_loss': 3.2772, 'train_time': 1301740}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2772, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 5100, 'val_loss': 3.2751, 'train_time': 949528}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2751, reaching under the 3.28 target validation loss.'}\n",
      "agent v_3: {'val_loss': 3.2757, 'train_time': 922823}\n",
      "====== record_5_20250429_084906_2142687-2142655-4, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 5100, 'val_loss': 3.2751, 'train_time': 949528}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2751, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 5100, 'val_loss': 3.275, 'train_time': 766259}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.275, reaching under the 3.28 target validation loss.'}\n",
      "agent v_8: {'val_loss': 3.2796, 'train_time': 765102}\n",
      "====== record_5_20250502_004532_2182811-2182655-4, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 5100, 'val_loss': 3.2751, 'train_time': 949528}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2751, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 5100, 'val_loss': 3.275, 'train_time': 766259}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.275, reaching under the 3.28 target validation loss.'}\n",
      "agent v_19: {'val_loss': 3.2796, 'train_time': 765580}\n",
      "====== record_5_20250502_004532_2182811-2182655-4, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 5100, 'val_loss': 3.2751, 'train_time': 949528}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2751, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 5100, 'val_loss': 3.275, 'train_time': 766259}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.275, reaching under the 3.28 target validation loss.'}\n",
      "agent v_10: {'val_loss': 3.2753, 'train_time': 764971}\n",
      "====== record_12_20250422_112536_2071550-2071548-1, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 1875, 'val_loss': 3.2739, 'train_time': 317839}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2739 in 5.23 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 1750, 'val_loss': 3.2739, 'train_time': 289805}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2739 in 4.76 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_73: {'val_loss': 3.2747, 'train_time': 288319}\n",
      "====== record_1_20250427_165133_2116540-2116462-0, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_31: {'val_loss': 3.2779071807861326, 'train_time': 2204110}\n",
      "====== record_1_20250427_165133_2116540-2116462-0, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_19: {'val_loss': 3.277819299697876, 'train_time': 2183229}\n",
      "====== record_1_20250502_004537_2183029-2182666-0, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_18: {'val_loss': 3.2779615759849547, 'train_time': 2209575}\n",
      "====== record_1_20250502_004537_2183029-2182666-0, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_16: {'val_loss': 3.2776796221733093, 'train_time': 2194751}\n",
      "====== record_10_20250427_165133_2116462-2116462-9, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2782, 'train_time': 477150}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3242, 'val_loss': 3.2742, 'train_time': 442985}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2742 in 7.29 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_26: {'val_loss': 3.2753, 'train_time': 442441}\n",
      "====== record_10_20250427_165133_2116462-2116462-9, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2782, 'train_time': 477150}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3242, 'val_loss': 3.2742, 'train_time': 442985}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2742 in 7.29 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_15: {'val_loss': 3.2744, 'train_time': 441249}\n",
      "====== record_9_20250425_091454_2098949-2098940-8, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2785, 'train_time': 505531}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2782, 'train_time': 477150}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_68: {'val_loss': 3.2779, 'train_time': 475936}\n",
      "====== record_9_20250425_091454_2098949-2098940-8, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2785, 'train_time': 505531}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2782, 'train_time': 477150}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_81: {'val_loss': 3.2798, 'train_time': 474568}\n",
      "====== record_9_20250425_091454_2098949-2098940-8, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2785, 'train_time': 505531}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2782, 'train_time': 477150}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_76: {'val_loss': 3.2796, 'train_time': 471196}\n",
      "====== record_1_20250427_165058_2116485-2116457-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_32: {'val_loss': 3.2797, 'train_time': 2200211}\n",
      "====== record_1_20250427_165058_2116485-2116457-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_28: {'val_loss': 3.278716814517975, 'train_time': 2196640}\n",
      "====== record_5_20250427_165058_2116534-2116457-4, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 5100, 'val_loss': 3.2751, 'train_time': 949528}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2751, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 5100, 'val_loss': 3.275, 'train_time': 766259}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.275, reaching under the 3.28 target validation loss.'}\n",
      "agent v_22: {'val_loss': 3.2762, 'train_time': 765916}\n",
      "====== record_1_20250502_004532_2182834-2182656-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_18: {'val_loss': 3.278874123096466, 'train_time': 2206118}\n",
      "====== record_1_20250502_004532_2182834-2182656-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_19: {'val_loss': 3.2790470838546755, 'train_time': 2203254}\n",
      "====== record_12_20250427_165012_2116479-2116450-1, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 1875, 'val_loss': 3.2739, 'train_time': 317839}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2739 in 5.23 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 1750, 'val_loss': 3.2739, 'train_time': 289805}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2739 in 4.76 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_15: {'val_loss': 3.2772, 'train_time': 289350}\n",
      "====== record_9_20250502_004532_2182842-2182656-8, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2785, 'train_time': 505531}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2782, 'train_time': 477150}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_6: {'val_loss': 3.2787, 'train_time': 476284}\n",
      "====== record_10_20250427_164419_2116288-2116288-9, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2782, 'train_time': 477150}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3242, 'val_loss': 3.2742, 'train_time': 442985}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2742 in 7.29 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_2: {'val_loss': 3.278, 'train_time': 440097}\n",
      "====== record_15_20250412_122737_1968007-1968001-4, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 1480, 'val_loss': 3.2771, 'train_time': 241463}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2771 in 4.02 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 1480, 'val_loss': 3.2773, 'train_time': 232971}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2773 in 3.88 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_12: {'val_loss': 0.0372, 'train_time': 227203}\n",
      "====== record_15_20250412_122737_1968007-1968001-4, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 1480, 'val_loss': 3.2771, 'train_time': 241463}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2771 in 4.02 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 1480, 'val_loss': 3.2773, 'train_time': 232971}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2773 in 3.88 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_20: {'val_loss': 0.0371, 'train_time': 226025}\n",
      "====== record_5_20250427_164404_2116264-2116258-4, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 5100, 'val_loss': 3.2751, 'train_time': 949528}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2751, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 5100, 'val_loss': 3.275, 'train_time': 766259}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.275, reaching under the 3.28 target validation loss.'}\n",
      "agent v_8: {'val_loss': 3.2755, 'train_time': 764418}\n",
      "====== record_9_20250425_091315_2098906-2098896-8, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2785, 'train_time': 505531}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2782, 'train_time': 477150}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_8: {'val_loss': 3.2788, 'train_time': 475931}\n",
      "====== record_1_20250425_091315_2098898-2098896-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_21: {'val_loss': 3.279305577278137, 'train_time': 2205347}\n",
      "====== record_1_20250427_164404_2116260-2116258-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_17: {'val_loss': 3.277691125869751, 'train_time': 2209525}\n",
      "====== record_1_20250427_164404_2116260-2116258-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_21: {'val_loss': 3.276810610294342, 'train_time': 2207004}\n",
      "====== record_1_20250414_114046_1975483-1975482-0, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_28: {'val_loss': 3.2757948279380797, 'train_time': 2207292}\n",
      "====== record_1_20250414_114046_1975483-1975482-0, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_23: {'val_loss': 3.2778238654136658, 'train_time': 2205406}\n",
      "====== record_10_20250422_112411_2071515-2071515-9, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2782, 'train_time': 477150}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3242, 'val_loss': 3.2742, 'train_time': 442985}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2742 in 7.29 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_72: {'val_loss': 3.2771, 'train_time': 441087}\n",
      "====== record_1_20250422_112411_2071516-2071515-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_51: {'val_loss': 3.278363275527954, 'train_time': 2204213}\n",
      "====== record_1_20250422_112411_2071516-2071515-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_40: {'val_loss': 3.278217685222626, 'train_time': 2195135}\n",
      "====== record_1_20250502_004532_2182843-2182657-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_14: {'val_loss': 3.2789417266845704, 'train_time': 2199152}\n",
      "====== record_1_20250502_004533_2182878-2182658-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_17: {'val_loss': 3.2750123620033262, 'train_time': 2205518}\n",
      "====== record_1_20250416_094544_1992851-1992631-0, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_35: {'val_loss': 3.27721266746521, 'train_time': 2207665}\n",
      "====== record_1_20250416_094544_1992851-1992631-0, level 12 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_48: {'val_loss': 3.275292360782623, 'train_time': 2188124}\n",
      "====== record_1_20250424_065837_2088652-2088441-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_19: {'val_loss': 3.2784698843955993, 'train_time': 2207639}\n",
      "====== record_1_20250424_065837_2088652-2088441-0, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 24576, 'val_loss': 3.2766, 'train_time': 2968348}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2766 in 48.94 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 9536, 'val_loss': 3.2603, 'train_time': 2209926}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2603, reaching under the 3.28 target validation loss.'}\n",
      "agent v_21: {'val_loss': 3.277706873416901, 'train_time': 2204435}\n",
      "====== record_12_20250424_065957_2089103-2088450-1, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 1875, 'val_loss': 3.2739, 'train_time': 317839}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2739 in 5.23 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 1750, 'val_loss': 3.2739, 'train_time': 289805}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2739 in 4.76 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_2: {'val_loss': 3.2732, 'train_time': 289589}\n",
      "====== record_10_20250424_065837_2088441-2088441-9, level 125 ======\n",
      "human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3200, 'val_loss': 3.2782, 'train_time': 477150}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2785 in 8.31 minutes, reaching under the 3.28 target validation loss.'}\n",
      "next human: {'job_status': 'COMPLETED', 'metrics': {'n_steps': 3242, 'val_loss': 3.2742, 'train_time': 442985}, 'hypothesis': 'Baseline run of GPT2 124M model on FineWeb 10B dataset with default hyperparameters.', 'outcome_summary': 'The model achieves a validation loss of 3.2742 in 7.29 minutes, reaching under the 3.28 target validation loss.'}\n",
      "agent v_9: {'val_loss': 3.2774, 'train_time': 441554}\n"
     ]
    }
   ],
   "source": [
    "# generate LLM judge prompts\n",
    "model_name = \"o3_mini\"\n",
    "with open(\n",
    "    # '/home/user123456/scientist/code_analysis_with_all_versions_knowledge.json', \n",
    "    f\"/home/user123456/scientist/code_analysis_with_all_versions_knowledge_{model_name}.json\",\n",
    "    'r'\n",
    ") as f:\n",
    "    record_dict_w_level = json.load(f)\n",
    "prompt_version = \"v4\"\n",
    "prompt = '''\n",
    "Below is a baseline implementation of a GPT-2 model, followed by two proposed changes (see code diffs below) to improve the training speed. \n",
    "The first change is from an expert human. The second change is from an AI Assistant, aiming to reproduce the improvement made by the expert human.\n",
    "Inspect the code diffs carefully and provide an objective evaluation of the AI Assistant's solution in terms of its similarity with expert human's solution. To derive an objective evaluation, first enumerate all the key changes made by expert human which can affect training speed, and then analyze all the changes made by the AI Assistant one by one. Based on understanding of these code changes, derive a percentage score (between 0 and 1) to quantify what fraction of the key changes (which has impact on training speed) made by the expert were correctly implemented in the AI Assistant's solution. \n",
    "\n",
    "Return your final score in a JSON object, with the key \"reproducibility_score\".\n",
    "# =============== Baseline Implementation ===========\n",
    "{human_code}\n",
    "# =============== Change made by Expert Human ===========\n",
    "{next_human_code}\n",
    "# =============== Change made by AI Assistant ===========\n",
    "{agent_code}\n",
    "'''\n",
    "score_prompts = []\n",
    "judge_name = \"DeepSeek-R1-Distill-Llama-70B\"\n",
    "judge_name = \"DeepSeek-R1\"\n",
    "for method, data in record_dict_w_level.items():\n",
    "    #if method != \"tree\":\n",
    "    #    continue\n",
    "    for level, records in data.items():\n",
    "        for target_record, run_data in records.items():\n",
    "            # print(target_record)\n",
    "            if \"human_code\" not in run_data.keys():\n",
    "                continue\n",
    "            next_human_code = run_data[\"human_diff\"] \n",
    "            if next_human_code == \"\":\n",
    "                continue\n",
    "            human_code = run_data[\"human_code\"]\n",
    "             \n",
    "            human_metrics =run_data[\"metrics\"]\n",
    "            next_human_metrics =run_data[\"next_metrics\"]\n",
    "            min_train_time = 100000000\n",
    "            for k, v in run_data.items():\n",
    "                if \"v_\" in k:\n",
    "                    if v[\"metrics\"][\"val_loss\"] is not None:\n",
    "                        if v[\"metrics\"][\"val_loss\"] > 0 and v[\"metrics\"][\"val_loss\"] <= 3.28 and v[\"metrics\"][\"train_time\"] < min_train_time and v[\"metrics\"][\"train_time\"] > 100000:\n",
    "                            min_train_time = v[\"metrics\"][\"train_time\"]\n",
    "                            agent_speedup = human_metrics[\"metrics\"][\"train_time\"] - v[\"metrics\"][\"train_time\"]\n",
    "                            human_speedup = human_metrics[\"metrics\"][\"train_time\"] - next_human_metrics[\"metrics\"][\"train_time\"]\n",
    "                            if agent_speedup > human_speedup :\n",
    "                                print(f\"====== {target_record}, level {level} ======\")\n",
    "                                print(f\"human: {human_metrics}\")\n",
    "                                print(f\"next human: {next_human_metrics}\")\n",
    "                                print(f\"agent {k}: {v[\"metrics\"]}\")\n",
    "                                \n",
    "      \n",
    "                            agent_code = v[\"version_diff\"]\n",
    "                            score_prompt = prompt.format(human_code=human_code, next_human_code=next_human_code, agent_code=agent_code)\n",
    "                            annot_data = {\n",
    "                                \"method\": method,\n",
    "                                \"level\": level,\n",
    "                                \"record\": target_record,\n",
    "                                \"human_metrics\": human_metrics,\n",
    "                                \"next_human_metrics\": next_human_metrics,\n",
    "                                \"version\": k,\n",
    "                                \"metrics\": v[\"metrics\"],\n",
    "                                \"model\": run_data[\"model\"],\n",
    "                            }\n",
    "                            if judge_name == \"DeepSeek-R1-Distill-Llama-70B\":\n",
    "                                annot_data[\"text\"] =  f\"<｜User｜>{score_prompt}<｜Assistant｜>\"\n",
    "                            else:\n",
    "                                annot_data[\"score_prompt\"] = score_prompt\n",
    "                                \n",
    "            score_prompts.append(annot_data)\n",
    "\n",
    "save_to_jsonl(score_prompts, f\"/home/user123457/scientist/llm_judge_prompts/all_records_metadata_best_{model_name}_prompts_{prompt_version}_{judge_name}.jsonl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483d651-4544-4cc4-bf8b-08c78b12ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annoated results\n",
    "data = load_from_jsonl(\n",
    "    #\"/home/user123457/scientist/llm_judge_prompts/all_records_metadata_best_code_diff_annot_DeepSeek-R1-Distill-Llama-70B_v3.jsonl\"\n",
    "    \"/home/user123457/scientist/llm_judge_prompts/all_records_metadata_best_o3_mini_code_diff_annot_DeepSeek-R1-Distill-Llama-70B_v4.jsonl\"\n",
    ")\n",
    "\n",
    "\n",
    "def extract_json_from_string(text: str) -> dict | list | None:\n",
    "    \"\"\"\n",
    "    Extracts a JSON object or array embedded within Markdown code fences\n",
    "    (specifically ```json ... ```) from a string.\n",
    "\n",
    "    Args:\n",
    "        text: The input string potentially containing the JSON in code fences.\n",
    "\n",
    "    Returns:\n",
    "        The parsed JSON object (as a dict) or array (as a list),\n",
    "        or None if no valid JSON block is found or parsing fails.\n",
    "    \"\"\"\n",
    "    # Regex to find content within ```json ... ``` block\n",
    "    # - ```json : Matches the start fence literally\n",
    "    # - \\s*    : Matches optional whitespace (including newline) after 'json'\n",
    "    # - (.*?)  : Captures the content non-greedily (*) between fences.\n",
    "    #            The '.' matches any character, '?' makes it non-greedy.\n",
    "    # - \\s*    : Matches optional whitespace before the end fence\n",
    "    # - ```    : Matches the end fence literally\n",
    "    # re.DOTALL flag makes '.' match newline characters as well.\n",
    "    match = re.search(r'```json\\s*(.*?)\\s*```', text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        json_string = match.group(1).strip() # Extract captured group and strip leading/trailing whitespace\n",
    "        try:\n",
    "            # Attempt to parse the extracted string as JSON\n",
    "            data = json.loads(json_string)\n",
    "            return data\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            print(f\"Extracted string was: '{json_string}'\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during JSON parsing: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        # print(\"No ```json ... ``` block found in the string.\")\n",
    "        return None\n",
    "\n",
    "for i, datum in enumerate(data):\n",
    "    data[i][\"record_number\"] = datum[\"record\"].split(\"_\")[1]\n",
    "    try:\n",
    "        input_string = datum[\"vllm_output\"][\"output\"].split(\"</think>\")[1]\n",
    "        extracted_data = extract_json_from_string(input_string)\n",
    "\n",
    "        if extracted_data:\n",
    "            # print(\"Successfully extracted JSON:\")\n",
    "            # print(extracted_data)\n",
    "            data[i][\"code_diff_judge\"] = extracted_data\n",
    "            # print(f\"Correctness Score: {extracted_data.get('correctness_score')}\")\n",
    "    except Exception as e:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aa35fd1-cb28-446f-96b7-45b61dc0d5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting data processing...\n",
      "INFO:__main__:Initial DataFrame created with 479 rows.\n",
      "INFO:__main__:Starting deduplication based on minimum 'train_time'...\n",
      "INFO:__main__:Deduplication complete. Kept 182 rows (dropped 297 duplicates).\n",
      "INFO:__main__:Preprocessing complete. 182 valid rows remaining.\n",
      "INFO:__main__:Plotting methods in order: ['tree', 'forest', 'flat', 'ori_aide', 'multi_aide']\n",
      "INFO:__main__:Calculated global Y-limits for 'reproducibility_score': (-0.05, 1.05)\n",
      "INFO:__main__:Generating subplot 0 for method: tree\n",
      "/tmp/ipykernel_1031299/3410857662.py:176: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=method_df, x='record_number', y=y_col, hue='model_level_group', ax=ax, ci=None)\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:__main__:Generating subplot 1 for method: forest\n",
      "/tmp/ipykernel_1031299/3410857662.py:176: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=method_df, x='record_number', y=y_col, hue='model_level_group', ax=ax, ci=None)\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "DEBUG: DataFrame used for grouped bar plots:\n",
      "       model      method level record_number_str  record_number  train_time  reproducibility_score model_level_group\n",
      "452  o3-mini        flat    12                 1              1     2188124                  0.920     o3-mini | L12\n",
      "101  o3-mini      forest    12                 1              1     2183229                  0.750     o3-mini | L12\n",
      "302  o3-mini  multi_aide    12                 1              1     2205406                  0.950     o3-mini | L12\n",
      "235  o3-mini    ori_aide    12                 1              1     2229536                  0.980     o3-mini | L12\n",
      "9    o3-mini        tree    12                 1              1     2221258                  0.700     o3-mini | L12\n",
      "467  o3-mini        flat   125                 1              1     2204435                  0.400    o3-mini | L125\n",
      "163  o3-mini      forest   125                 1              1     2196640                  0.100    o3-mini | L125\n",
      "365  o3-mini  multi_aide   125                 1              1     2195135                  0.950    o3-mini | L125\n",
      "280  o3-mini    ori_aide   125                 1              1     2205347                  0.900    o3-mini | L125\n",
      "68   o3-mini        tree   125                 1              1     2346402                  0.650    o3-mini | L125\n",
      "447  o3-mini        flat    12                 2              2     1674697                  0.300     o3-mini | L12\n",
      "131  o3-mini      forest    12                 2              2     1661106                  0.200     o3-mini | L12\n",
      "331  o3-mini  multi_aide    12                 2              2     1659197                  0.850     o3-mini | L12\n",
      "238  o3-mini    ori_aide    12                 2              2     1673243                  0.200     o3-mini | L12\n",
      "26   o3-mini        tree    12                 2              2     1672115                  0.600     o3-mini | L12\n",
      "477  o3-mini        flat   125                 2              2     1670789                  0.950    o3-mini | L125\n",
      "160  o3-mini      forest   125                 2              2     1663606                  0.850    o3-mini | L125\n",
      "401  o3-mini  multi_aide   125                 2              2     1664684                  0.850    o3-mini | L125\n",
      "265  o3-mini    ori_aide   125                 2              2     1678546                  0.950    o3-mini | L125\n",
      "80   o3-mini        tree   125                 2              2     1664947                  0.700    o3-mini | L125\n",
      "459  o3-mini        flat    12                 3              3     1423531                  0.850     o3-mini | L12\n",
      "114  o3-mini      forest    12                 3              3     1420825                  0.850     o3-mini | L12\n",
      "294  o3-mini  multi_aide    12                 3              3     1421896                  0.900     o3-mini | L12\n",
      "225  o3-mini    ori_aide    12                 3              3     1353634                  0.700     o3-mini | L12\n",
      "12   o3-mini        tree    12                 3              3     1424630                  0.700     o3-mini | L12\n",
      "465  o3-mini        flat   125                 3              3     1416765                  0.700    o3-mini | L125\n",
      "169  o3-mini      forest   125                 3              3     1417400                  0.850    o3-mini | L125\n",
      "393  o3-mini  multi_aide   125                 3              3     1414157                    NaN    o3-mini | L125\n",
      "264  o3-mini    ori_aide   125                 3              3     1422420                  0.800    o3-mini | L125\n",
      "66   o3-mini        tree   125                 3              3     1418907                  0.333    o3-mini | L125\n",
      "461  o3-mini        flat    12                 4              4     1288662                  0.250     o3-mini | L12\n",
      "124  o3-mini      forest    12                 4              4     1017681                  0.750     o3-mini | L12\n",
      "332  o3-mini  multi_aide    12                 4              4     1065162                  1.000     o3-mini | L12\n",
      "244  o3-mini    ori_aide    12                 4              4     1258967                  0.900     o3-mini | L12\n",
      "18   o3-mini        tree    12                 4              4      922823                  0.900     o3-mini | L12\n",
      "462  o3-mini        flat   125                 4              4     1022168                  0.700    o3-mini | L125\n",
      "170  o3-mini      forest   125                 4              4     1009475                  1.000    o3-mini | L125\n",
      "378  o3-mini  multi_aide   125                 4              4     1008012                  0.950    o3-mini | L125\n",
      "271  o3-mini    ori_aide   125                 4              4      993392                  0.850    o3-mini | L125\n",
      "90   o3-mini        tree   125                 4              4     1301740                  0.900    o3-mini | L125\n",
      "449  o3-mini        flat    12                 5              5      949528                  0.900     o3-mini | L12\n",
      "105  o3-mini      forest    12                 5              5      773010                  0.600     o3-mini | L12\n",
      "307  o3-mini  multi_aide    12                 5              5      777745                  0.600     o3-mini | L12\n",
      "242  o3-mini    ori_aide    12                 5              5      781611                  0.700     o3-mini | L12\n",
      "39   o3-mini        tree    12                 5              5      765102                  0.700     o3-mini | L12\n",
      "475  o3-mini        flat   125                 5              5      769450                  0.500    o3-mini | L125\n",
      "167  o3-mini      forest   125                 5              5      765916                  0.600    o3-mini | L125\n",
      "434  o3-mini  multi_aide   125                 5              5      773819                  0.700    o3-mini | L125\n",
      "252  o3-mini    ori_aide   125                 5              5      764418                  0.800    o3-mini | L125\n",
      "54   o3-mini        tree   125                 5              5      764971                  0.500    o3-mini | L125\n",
      "448  o3-mini        flat    12                 7              7      697604                  0.850     o3-mini | L12\n",
      "127  o3-mini      forest    12                 7              7      695689                  0.800     o3-mini | L12\n",
      "289  o3-mini  multi_aide    12                 7              7      694385                  0.700     o3-mini | L12\n",
      "218  o3-mini    ori_aide    12                 7              7      697495                  0.800     o3-mini | L12\n",
      "25   o3-mini        tree    12                 7              7      692746                  0.900     o3-mini | L12\n",
      "463  o3-mini        flat   125                 7              7      697586                  0.860    o3-mini | L125\n",
      "188  o3-mini      forest   125                 7              7      690515                  0.900    o3-mini | L125\n",
      "408  o3-mini  multi_aide   125                 7              7      693491                  0.950    o3-mini | L125\n",
      "255  o3-mini    ori_aide   125                 7              7      695931                  0.600    o3-mini | L125\n",
      "49   o3-mini        tree   125                 7              7      693895                  0.900    o3-mini | L125\n",
      "456  o3-mini        flat    12                 8              8      662205                  0.875     o3-mini | L12\n",
      "104  o3-mini      forest    12                 8              8      662205                  0.950     o3-mini | L12\n",
      "291  o3-mini  multi_aide    12                 8              8      662205                  0.950     o3-mini | L12\n",
      "213  o3-mini    ori_aide    12                 8              8      662205                  0.600     o3-mini | L12\n",
      "2    o3-mini        tree    12                 8              8      662205                  0.700     o3-mini | L12\n",
      "470  o3-mini        flat   125                 8              8      662205                  0.950    o3-mini | L125\n",
      "161  o3-mini      forest   125                 8              8      662205                  0.600    o3-mini | L125\n",
      "344  o3-mini  multi_aide   125                 8              8      662205                  0.700    o3-mini | L125\n",
      "248  o3-mini    ori_aide   125                 8              8      662205                  0.950    o3-mini | L125\n",
      "52   o3-mini        tree   125                 8              8      662205                  0.950    o3-mini | L125\n",
      "454  o3-mini        flat    12                 9              9      505531                  0.950     o3-mini | L12\n",
      "99   o3-mini      forest    12                 9              9      505531                  0.950     o3-mini | L12\n",
      "336  o3-mini  multi_aide    12                 9              9      489620                  0.900     o3-mini | L12\n",
      "224  o3-mini    ori_aide    12                 9              9      505531                  0.700     o3-mini | L12\n",
      "5    o3-mini        tree    12                 9              9      505531                  0.900     o3-mini | L12\n",
      "478  o3-mini        flat   125                 9              9      505531                  0.800    o3-mini | L125\n",
      "142  o3-mini      forest   125                 9              9      471196                  0.900    o3-mini | L125\n",
      "422  o3-mini  multi_aide   125                 9              9      477651                  0.900    o3-mini | L125\n",
      "263  o3-mini    ori_aide   125                 9              9      475931                  0.600    o3-mini | L125\n",
      "91   o3-mini        tree   125                 9              9      483114                  0.700    o3-mini | L125\n",
      "453  o3-mini        flat    12                10             10      443678                  0.850     o3-mini | L12\n",
      "122  o3-mini      forest    12                10             10      441249                  0.600     o3-mini | L12\n",
      "329  o3-mini  multi_aide    12                10             10      451458                  0.900     o3-mini | L12\n",
      "231  o3-mini    ori_aide    12                10             10      440097                  0.700     o3-mini | L12\n",
      "36   o3-mini        tree    12                10             10      468753                  0.700     o3-mini | L12\n",
      "473  o3-mini        flat   125                10             10      441554                  0.900    o3-mini | L125\n",
      "145  o3-mini      forest   125                10             10      447750                  0.750    o3-mini | L125\n",
      "357  o3-mini  multi_aide   125                10             10      441087                  0.600    o3-mini | L125\n",
      "275  o3-mini    ori_aide   125                10             10      444436                  0.750    o3-mini | L125\n",
      "59   o3-mini        tree   125                10             10      444756                  0.900    o3-mini | L125\n",
      "446  o3-mini        flat    12                11             11      433087                  0.700     o3-mini | L12\n",
      "125  o3-mini      forest    12                11             11      432810                  0.600     o3-mini | L12\n",
      "318  o3-mini  multi_aide    12                11             11      432697                  0.400     o3-mini | L12\n",
      "219  o3-mini    ori_aide    12                11             11      442985                  0.700     o3-mini | L12\n",
      "23   o3-mini        tree    12                11             11      435097                  0.400     o3-mini | L12\n",
      "466  o3-mini        flat   125                11             11      433544                  0.700    o3-mini | L125\n",
      "149  o3-mini      forest   125                11             11      431574                  0.300    o3-mini | L125\n",
      "350  o3-mini  multi_aide   125                11             11      433100                  0.700    o3-mini | L125\n",
      "287  o3-mini    ori_aide   125                11             11      432477                  0.850    o3-mini | L125\n",
      "82   o3-mini        tree   125                11             11      432466                  0.600    o3-mini | L125\n",
      "455  o3-mini        flat    12                12             12      313798                  0.700     o3-mini | L12\n",
      "121  o3-mini      forest    12                12             12      312901                  0.000     o3-mini | L12\n",
      "304  o3-mini  multi_aide    12                12             12      295170                  0.750     o3-mini | L12\n",
      "239  o3-mini    ori_aide    12                12             12      314618                  0.625     o3-mini | L12\n",
      "35   o3-mini        tree    12                12             12      306445                  0.800     o3-mini | L12\n",
      "471  o3-mini        flat   125                12             12      289589                  0.700    o3-mini | L125\n",
      "191  o3-mini      forest   125                12             12      289350                  0.850    o3-mini | L125\n",
      "436  o3-mini  multi_aide   125                12             12      290157                  0.750    o3-mini | L125\n",
      "249  o3-mini    ori_aide   125                12             12      314166                  0.700    o3-mini | L125\n",
      "76   o3-mini        tree   125                12             12      288319                  0.800    o3-mini | L125\n",
      "460  o3-mini        flat    12                13             13      289325                  0.700     o3-mini | L12\n",
      "108  o3-mini      forest    12                13             13      288080                  0.200     o3-mini | L12\n",
      "328  o3-mini  multi_aide    12                13             13      289805                  0.700     o3-mini | L12\n",
      "212  o3-mini    ori_aide    12                13             13      289805                  0.950     o3-mini | L12\n",
      "8    o3-mini        tree    12                13             13      289805                  0.700     o3-mini | L12\n",
      "468  o3-mini        flat   125                13             13      287943                  0.950    o3-mini | L125\n",
      "193  o3-mini      forest   125                13             13      285234                  0.050    o3-mini | L125\n",
      "353  o3-mini  multi_aide   125                13             13      289237                  0.700    o3-mini | L125\n",
      "284  o3-mini    ori_aide   125                13             13      289397                  0.600    o3-mini | L125\n",
      "89   o3-mini        tree   125                13             13      288212                  0.200    o3-mini | L125\n",
      "457  o3-mini        flat    12                14             14      273107                  0.950     o3-mini | L12\n",
      "118  o3-mini      forest    12                14             14      273107                  0.700     o3-mini | L12\n",
      "314  o3-mini  multi_aide    12                14             14      273107                  0.850     o3-mini | L12\n",
      "216  o3-mini    ori_aide    12                14             14      273107                  0.600     o3-mini | L12\n",
      "0    o3-mini        tree    12                14             14      273107                  0.300     o3-mini | L12\n",
      "472  o3-mini        flat   125                14             14      264756                  0.350    o3-mini | L125\n",
      "211  o3-mini      forest   125                14             14      263500                  0.200    o3-mini | L125\n",
      "354  o3-mini  multi_aide   125                14             14      260253                  0.450    o3-mini | L125\n",
      "259  o3-mini    ori_aide   125                14             14      265054                  0.400    o3-mini | L125\n",
      "67   o3-mini        tree   125                14             14      265541                  0.200    o3-mini | L125\n",
      "451  o3-mini        flat    12                15             15      237850                  0.670     o3-mini | L12\n",
      "135  o3-mini      forest    12                15             15      237738                  0.600     o3-mini | L12\n",
      "298  o3-mini  multi_aide    12                15             15      237766                  0.000     o3-mini | L12\n",
      "233  o3-mini    ori_aide    12                15             15      226025                  0.700     o3-mini | L12\n",
      "27   o3-mini        tree    12                15             15      238362                  0.200     o3-mini | L12\n",
      "464  o3-mini        flat   125                15             15      239173                  0.800    o3-mini | L125\n",
      "152  o3-mini      forest   125                15             15      237842                  0.600    o3-mini | L125\n",
      "416  o3-mini  multi_aide   125                15             15      237372                  0.200    o3-mini | L125\n",
      "277  o3-mini    ori_aide   125                15             15      238310                  0.900    o3-mini | L125\n",
      "92   o3-mini        tree   125                15             15      238123                  0.400    o3-mini | L125\n",
      "458  o3-mini        flat    12                16             16      229263                  0.200     o3-mini | L12\n",
      "110  o3-mini      forest    12                16             16      229180                  0.500     o3-mini | L12\n",
      "303  o3-mini  multi_aide    12                16             16      228187                  0.200     o3-mini | L12\n",
      "220  o3-mini    ori_aide    12                16             16      229677                  0.300     o3-mini | L12\n",
      "6    o3-mini        tree    12                16             16      228909                  0.200     o3-mini | L12\n",
      "474  o3-mini        flat   125                16             16      229758                  0.950    o3-mini | L125\n",
      "177  o3-mini      forest   125                16             16      228580                  0.250    o3-mini | L125\n",
      "438  o3-mini  multi_aide   125                16             16      229695                  0.400    o3-mini | L125\n",
      "257  o3-mini    ori_aide   125                16             16      228763                  0.500    o3-mini | L125\n",
      "65   o3-mini        tree   125                16             16      222835                  0.200    o3-mini | L125\n",
      "445  o3-mini        flat    12                17             17      217967                  0.700     o3-mini | L12\n",
      "123  o3-mini      forest    12                17             17      216562                  0.100     o3-mini | L12\n",
      "305  o3-mini  multi_aide    12                17             17      216394                  0.300     o3-mini | L12\n",
      "230  o3-mini    ori_aide    12                17             17      216826                  0.200     o3-mini | L12\n",
      "11   o3-mini        tree    12                17             17      218922                  0.100     o3-mini | L12\n",
      "469  o3-mini        flat   125                17             17      217916                  0.700    o3-mini | L125\n",
      "153  o3-mini      forest   125                17             17      216586                  0.200    o3-mini | L125\n",
      "356  o3-mini  multi_aide   125                17             17      216393                  0.750    o3-mini | L125\n",
      "270  o3-mini    ori_aide   125                17             17      216341                  0.850    o3-mini | L125\n",
      "58   o3-mini        tree   125                17             17      216721                  0.200    o3-mini | L125\n",
      "450  o3-mini        flat    12                18             18      207846                  0.850     o3-mini | L12\n",
      "102  o3-mini      forest    12                18             18      208448                  0.900     o3-mini | L12\n",
      "340  o3-mini  multi_aide    12                18             18      207705                  0.850     o3-mini | L12\n",
      "214  o3-mini    ori_aide    12                18             18      211840                  0.850     o3-mini | L12\n",
      "17   o3-mini        tree    12                18             18      211840                  0.700     o3-mini | L12\n",
      "476  o3-mini        flat   125                18             18      208669                  0.800    o3-mini | L125\n",
      "187  o3-mini      forest   125                18             18      207221                  0.300    o3-mini | L125\n",
      "385  o3-mini  multi_aide   125                18             18      209005                  0.150    o3-mini | L125\n",
      "260  o3-mini    ori_aide   125                18             18      208459                  0.900    o3-mini | L125\n",
      "84   o3-mini        tree   125                18             18      208277                  0.900    o3-mini | L125\n",
      "130  o3-mini      forest    12                19             19      199442                  1.000     o3-mini | L12\n",
      "312  o3-mini  multi_aide    12                19             19      199151                  0.100     o3-mini | L12\n",
      "19   o3-mini        tree    12                19             19      199442                  0.800     o3-mini | L12\n",
      "162  o3-mini      forest   125                19             19      196738                  0.200    o3-mini | L125\n",
      "361  o3-mini  multi_aide   125                19             19      198106                  0.100    o3-mini | L125\n",
      "72   o3-mini        tree   125                19             19      198799                  0.700    o3-mini | L125\n",
      "100  o3-mini      forest    12                20             20      187662                  0.700     o3-mini | L12\n",
      "308  o3-mini  multi_aide    12                20             20      186944                  0.200     o3-mini | L12\n",
      "29   o3-mini        tree    12                20             20      187710                  0.400     o3-mini | L12\n",
      "147  o3-mini      forest   125                20             20      188476                  0.600    o3-mini | L125\n",
      "423  o3-mini  multi_aide   125                20             20      187111                  0.200    o3-mini | L125\n",
      "97   o3-mini        tree   125                20             20      187762                  0.300    o3-mini | L125\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating subplot 2 for method: flat\n",
      "/tmp/ipykernel_1031299/3410857662.py:176: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=method_df, x='record_number', y=y_col, hue='model_level_group', ax=ax, ci=None)\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:__main__:Generating subplot 3 for method: ori_aide\n",
      "/tmp/ipykernel_1031299/3410857662.py:176: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=method_df, x='record_number', y=y_col, hue='model_level_group', ax=ax, ci=None)\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:__main__:Generating subplot 4 for method: multi_aide\n",
      "/tmp/ipykernel_1031299/3410857662.py:176: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=method_df, x='record_number', y=y_col, hue='model_level_group', ax=ax, ci=None)\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:__main__:Placed legend in subplot slot 5\n",
      "INFO:__main__:Plot successfully saved to: methods_reproducibility_bars.pdf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "import os # Import os for path joining\n",
    "\n",
    "# --- Setup Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# --- Data Processing Function (no changes needed) ---\n",
    "def preprocess_data(data_list: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts list of dicts to DataFrame, extracts scores, keeps only the row\n",
    "    with min 'train_time' per group ('model','method','level','record_number'),\n",
    "    adds a combined model-level group, and prepares data for plotting.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    base_keys = ['model', 'method', 'level', 'record_number']\n",
    "    # score_keys = ['reproducibility_score', 'correctness_score', 'efficiency_score']\n",
    "    score_keys = ['reproducibility_score']\n",
    "\n",
    "    for i, item in enumerate(data_list):\n",
    "        if not isinstance(item, dict): continue\n",
    "        if not all(key in item for key in base_keys): continue\n",
    "\n",
    "        metrics_info = item.get('metrics', {})\n",
    "        judge_info = item.get('code_diff_judge', {})\n",
    "\n",
    "        train_time = metrics_info.get('train_time') if isinstance(metrics_info, dict) else None\n",
    "\n",
    "        scores = {key: None for key in score_keys}\n",
    "        if isinstance(judge_info, dict):\n",
    "            for key in score_keys: scores[key] = judge_info.get(key)\n",
    "\n",
    "        record_num = item.get('record_number')\n",
    "        model = item.get('model', 'Unknown')\n",
    "        level = item.get('level', 'Unknown')\n",
    "\n",
    "        record_data = {\n",
    "            'model': model, 'method': item.get('method'), 'level': level,\n",
    "            'record_number_str': str(record_num), 'record_number': record_num,\n",
    "            'train_time': train_time,\n",
    "        }\n",
    "        record_data.update(scores)\n",
    "        records.append(record_data)\n",
    "\n",
    "    if not records: return pd.DataFrame()\n",
    "    df = pd.DataFrame(records)\n",
    "    logger.info(f\"Initial DataFrame created with {len(df)} rows.\")\n",
    "\n",
    "    numeric_cols = ['record_number', 'train_time'] + score_keys\n",
    "    for col in numeric_cols: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    grouping_cols = ['model', 'method', 'level', 'record_number']\n",
    "    selection_key = 'train_time'\n",
    "    required_numeric_cols = ['record_number', selection_key]\n",
    "\n",
    "    initial_rows = len(df)\n",
    "    df.dropna(subset=required_numeric_cols, inplace=True)\n",
    "    rows_dropped_nan = initial_rows - len(df)\n",
    "    if rows_dropped_nan > 0: logger.info(f\"Dropped {rows_dropped_nan} rows due to missing required numeric values ({required_numeric_cols}).\")\n",
    "    if df.empty: return df\n",
    "\n",
    "    df['record_number'] = df['record_number'].astype(int)\n",
    "    df['model'] = df['model'].astype(str)\n",
    "    df['method'] = df['method'].astype(str)\n",
    "    df['level'] = df['level'].astype(str) # Ensure level is string for combination\n",
    "\n",
    "    logger.info(f\"Starting deduplication based on minimum '{selection_key}'...\")\n",
    "    initial_rows_before_dedup = len(df)\n",
    "    df.sort_values(by=grouping_cols + [selection_key], ascending=True, inplace=True)\n",
    "    df.drop_duplicates(subset=grouping_cols, keep='first', inplace=True)\n",
    "    rows_dropped_dedup = initial_rows_before_dedup - len(df)\n",
    "    logger.info(f\"Deduplication complete. Kept {len(df)} rows (dropped {rows_dropped_dedup} duplicates).\")\n",
    "\n",
    "    if df.empty: return df\n",
    "\n",
    "    # --- Create combined group for hue ---\n",
    "    df['model_level_group'] = df['model'] + ' | L' + df['level']\n",
    "\n",
    "    # Final sort for plotting consistency (method sort handled later)\n",
    "    df.sort_values(by=['record_number', 'model_level_group'], inplace=True)\n",
    "    logger.info(f\"Preprocessing complete. {len(df)} valid rows remaining.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Plotting Function (MODIFIED for order, legend placement, saving) ---\n",
    "def plot_method_grouped_bar_charts(df: pd.DataFrame,\n",
    "                                   y_col: str = 'reproducibility_score',\n",
    "                                   filename: str = \"method_bar_charts.pdf\"):\n",
    "    \"\"\"\n",
    "    Generates grouped bar chart subplots in a specified method order,\n",
    "    with a shared y-axis range, placing the legend in an empty subplot slot,\n",
    "    and saving the figure to a file.\n",
    "\n",
    "    Args:\n",
    "        df: The preprocessed DataFrame with 'model_level_group' column.\n",
    "        y_col: The name of the column containing the score to plot on the y-axis.\n",
    "        filename: The name (including path if needed) for the output PDF file.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logger.error(\"Cannot plot: DataFrame is empty.\")\n",
    "        return\n",
    "    required_cols = ['method', 'record_number', 'model_level_group', y_col]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        logger.error(f\"Cannot plot: DataFrame missing required columns ({required_cols}). Found: {df.columns.tolist()}\")\n",
    "        return\n",
    "\n",
    "    # --- Define Custom Method Order ---\n",
    "    method_order = ['tree', 'forest', 'flat', 'ori_aide', 'multi_aide']\n",
    "    available_methods_in_df = df['method'].unique()\n",
    "    # Filter and order methods based on definition and availability\n",
    "    methods_to_plot = [m for m in method_order if m in available_methods_in_df]\n",
    "    n_methods = len(methods_to_plot)\n",
    "\n",
    "    if n_methods == 0:\n",
    "        logger.error(f\"None of the desired methods ({method_order}) found in the DataFrame.\")\n",
    "        return\n",
    "    logger.info(f\"Plotting methods in order: {methods_to_plot}\")\n",
    "\n",
    "    # --- Calculate Global Y-Limits ---\n",
    "    global_ymin, global_ymax = None, None\n",
    "    valid_scores = df[y_col].dropna()\n",
    "    if not valid_scores.empty:\n",
    "        min_score, max_score = valid_scores.min(), valid_scores.max()\n",
    "        data_range = max_score - min_score\n",
    "        padding = (data_range * 0.05) if (pd.notna(data_range) and data_range > 1e-6) else 0.2\n",
    "        global_ymin = (min_score - padding) if pd.notna(min_score) else None\n",
    "        global_ymax = (max_score + padding) if pd.notna(max_score) else None\n",
    "        if pd.notna(min_score) and min_score >= 0:\n",
    "             negative_offset = max(padding * 0.1, 0.02 * (max_score if pd.notna(max_score) and max_score > 0 else 1))\n",
    "             if pd.notna(global_ymin): global_ymin = min(global_ymin, -negative_offset)\n",
    "             else: global_ymin = -negative_offset\n",
    "        ymin_str = f\"{global_ymin:.2f}\" if pd.notna(global_ymin) else \"auto\"\n",
    "        ymax_str = f\"{global_ymax:.2f}\" if pd.notna(global_ymax) else \"auto\"\n",
    "        logger.info(f\"Calculated global Y-limits for '{y_col}': ({ymin_str}, {ymax_str})\")\n",
    "    else:\n",
    "        logger.warning(f\"Could not calculate global Y-limits: No valid data found for '{y_col}'.\")\n",
    "\n",
    "    # --- Determine subplot layout (assuming 2 rows, 3 columns for 5 plots + legend) ---\n",
    "    ncols = 3\n",
    "    nrows = 2 # Fixed grid to accommodate 5 plots + legend slot\n",
    "    n_total_slots = nrows * ncols\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 5), squeeze=False, sharey=False)\n",
    "    axes = axes.flatten() # Flatten to 1D array\n",
    "\n",
    "    fig.suptitle(f'{y_col.replace(\"_\", \" \").title()} vs. Record Number (Grouped by Method, Bars by Model/Level)', fontsize=16, y=0.98)\n",
    "\n",
    "    last_plotted_ax_index = -1 # Track the index of the last axis actually used for a plot\n",
    "\n",
    "    # --- Iterate through methods IN CUSTOM ORDER ---\n",
    "    for i, method in enumerate(methods_to_plot):\n",
    "        ax = axes[i]\n",
    "        last_plotted_ax_index = i # Update the last plotted index\n",
    "        logger.info(f\"Generating subplot {i} for method: {method}\")\n",
    "\n",
    "        method_df = df[(df['method'] == method) & (df[y_col].notna())].copy()\n",
    "        method_df.sort_values(by=['record_number', 'model_level_group'], inplace=True)\n",
    "\n",
    "        if method_df.empty:\n",
    "            logger.warning(f\"No valid data found for method '{method}' and score '{y_col}'. Skipping subplot content.\")\n",
    "            ax.set_title(f'Method: {method}\\n(No Data for {y_col})')\n",
    "            if pd.notna(global_ymin) and pd.notna(global_ymax):\n",
    "                 ax.set_ylim(bottom=global_ymin, top=global_ymax)\n",
    "            ax.text(0.5, 0.5, f\"No valid '{y_col}' data\", ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue # Skip plotting for this method\n",
    "\n",
    "        # --- Create the grouped bar plot ---\n",
    "        sns.barplot(data=method_df, x='record_number', y=y_col, hue='model_level_group', ax=ax, ci=None)\n",
    "\n",
    "        # --- Apply GLOBAL Y-Limits ---\n",
    "        if pd.notna(global_ymin) and pd.notna(global_ymax):\n",
    "             ax.set_ylim(bottom=global_ymin, top=global_ymax)\n",
    "\n",
    "        # --- Customize Axes ---\n",
    "        ax.set_title(f'Method: {method}')\n",
    "        ax.set_ylabel(y_col.replace(\"_\", \" \").title())\n",
    "        ax.set_xlabel('Record Number')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        ax.legend().set_visible(False) # Hide individual subplot legends\n",
    "\n",
    "    # --- Legend Placement in Unused Slot ---\n",
    "    legend_ax_index = n_methods # The index of the first unused slot\n",
    "    if last_plotted_ax_index != -1 and legend_ax_index < n_total_slots:\n",
    "        handles, labels = axes[last_plotted_ax_index].get_legend_handles_labels()\n",
    "        if handles and labels:\n",
    "            # Place legend centered within the bounding box of the unused axis\n",
    "            legend_ax = axes[legend_ax_index]\n",
    "            legend_ax.axis('off') # Turn off the axis frame and ticks for the legend slot\n",
    "            fig.legend(handles, labels, title='Model | Level',\n",
    "                       bbox_to_anchor=legend_ax.get_position(), # Anchor to the legend axis position\n",
    "                       loc='center', # Center the legend within the anchor box\n",
    "                       bbox_transform=fig.transFigure, # Use figure coordinates for anchor box\n",
    "                       borderaxespad=0.)\n",
    "            logger.info(f\"Placed legend in subplot slot {legend_ax_index}\")\n",
    "        else:\n",
    "             logger.warning(f\"Could not generate legend: No handles/labels found on axis {last_plotted_ax_index}.\")\n",
    "             # Hide the legend axis anyway if it exists\n",
    "             if legend_ax_index < n_total_slots:\n",
    "                  axes[legend_ax_index].axis('off')\n",
    "    elif last_plotted_ax_index == -1:\n",
    "         logger.warning(\"Could not generate legend: No subplots contained data.\")\n",
    "         # Hide all potentially unused axes\n",
    "         for j in range(n_total_slots): axes[j].axis('off')\n",
    "\n",
    "    # --- Hide any other potentially unused axes beyond the legend slot ---\n",
    "    for j in range(legend_ax_index + 1, n_total_slots):\n",
    "         axes[j].axis('off')\n",
    "\n",
    "    # --- Adjust Layout and Save ---\n",
    "    # Use subplots_adjust for more control than tight_layout with manual legend\n",
    "    plt.subplots_adjust(left=0.05, right=0.95, bottom=0.1, top=0.90, wspace=0.3, hspace=0.4) # Tune these values\n",
    "\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(filename)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            logger.info(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "        plt.savefig(filename, bbox_inches='tight', format='pdf')\n",
    "        logger.info(f\"Plot successfully saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving plot to {filename}: {e}\", exc_info=True)\n",
    "    finally:\n",
    "        plt.close(fig) # Close the figure to free memory\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting data processing...\")\n",
    "    processed_df = preprocess_data(data)\n",
    "\n",
    "    output_filename = \"methods_reproducibility_bars.pdf\" # Define output filename\n",
    "\n",
    "    if not processed_df.empty:\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"DEBUG: DataFrame used for grouped bar plots:\")\n",
    "        print(processed_df.to_string())\n",
    "        print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "        plot_method_grouped_bar_charts(processed_df,\n",
    "                                       y_col='reproducibility_score',\n",
    "                                       filename=output_filename)\n",
    "    else:\n",
    "        logger.info(\"No plot generated as there was no valid data after preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc332eca-07c7-4442-a5d5-36c3ad00e813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
