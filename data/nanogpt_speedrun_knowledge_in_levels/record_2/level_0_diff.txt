diff --git a/temp_current.py b/temp_next.py
index 71ede92..6f0b3ee 100644
--- a/temp_current.py
+++ b/temp_next.py
@@ -1,7 +1,6 @@
 import os
 import sys
 import uuid
-import math
 import glob
 from dataclasses import dataclass
 
@@ -9,17 +8,91 @@ import numpy as np
 import torch
 from torch import nn
 import torch.nn.functional as F
+import torch.distributed as dist
 import torch._inductor.config as config
 from torch.nn.parallel import DistributedDataParallel as DDP
-from torch.distributed import init_process_group, destroy_process_group
 
 with open(sys.argv[0]) as f:
     code = f.read()
 
+# -----------------------------------------------------------------------------
+# OrthgonalNesterov optimizer
+
+class OrthogonalNesterov(torch.optim.Optimizer):
+    def __init__(self, params, lr=0.02, momentum=0.9, nesterov=True, zeropower_iters=5):
+        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, zeropower_iters=zeropower_iters)
+        super().__init__(params, defaults)
+
+    def step(self):
+        for group in self.param_groups:
+            lr = group['lr']
+            momentum = group['momentum']
+            for p in group['params']:
+                g = p.grad
+                if g is None:
+                    continue
+                state = self.state[p]
+                state['steps'] = state.get('steps', 0) + 1
+                if 'momentum_buffer' not in state:
+                    state['momentum_buffer'] = torch.zeros_like(g)
+                buf = state['momentum_buffer']
+                buf.mul_(momentum).add_(g)
+                g = g.add(buf, alpha=momentum) if group['nesterov'] else buf
+                update = zeroth_power_via_newtonschulz5(g, steps=group['zeropower_iters'])
+                p.data.add_(update, alpha=-lr)
+
+@torch.compile
+def zeroth_power_via_newtonschulz5(G, steps=5, eps=1e-7):
+    """
+    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
+    quintic iteration whose coefficients are selected to maximize the slope at zero. It turns out
+    to be empirically effective to keep increasing the slope of the quintic at zero even beyond the
+    point where it no longer converges to one everywhere after repeated application (so long as it
+    stays relatively close to 1 across the interval). Our usage of a Newton-Schulz iteration as the
+    orthogonalization method traces to Bernstein & Newhouse (2024) https://arxiv.org/abs/2409.20325
+    who suggested its use for computing the preconditioners of Shampoo.
+    """
+    assert len(G.shape) == 2
+    a, b, c = (3.4445, -4.7750,  2.0315)
+    X = G.bfloat16() / (G.norm() + eps) # ensure top singular value <= 1
+    if G.size(0) > G.size(1):
+        X = X.T
+    for _ in range(steps):
+        A = X @ X.T
+        B = A @ X
+        X = a * X + b * B + c * A @ B
+    if G.size(0) > G.size(1):
+        X = X.T
+    return X.to(G.dtype)
+
+class CombinedOptimizer:
+
+    def __init__(self, optimizers):
+        assert all(len(opt.param_groups) == 1 for opt in optimizers)
+        self.optimizers = optimizers
+        self.param_groups = [pg for opt in self.optimizers for pg in opt.param_groups]
+        self.base_lrs = [opt.param_groups[0]['lr'] for opt in self.optimizers]
+
+    def step(self):
+        for opt in self.optimizers:
+            opt.step()
+
+    def zero_grad(self, **kwargs):
+        for opt in self.optimizers:
+            opt.zero_grad(**kwargs)
+
+    def scale_lrs(self, lr_scale):
+        for base_lr, opt in zip(self.base_lrs, self.optimizers):
+            opt.param_groups[0]['lr'] = base_lr * lr_scale
+
+    def state_dict(self):
+        return [opt.state_dict() for opt in self.optimizers]
+
 # -----------------------------------------------------------------------------
 # PyTorch nn.Module definitions for the GPT-2 model
 
 class Rotary(torch.nn.Module):
+
     def __init__(self, dim, base=10000):
         super().__init__()
         inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
@@ -102,7 +175,7 @@ class Block(nn.Module):
         super().__init__()
         self.attn = CausalSelfAttention(config)
         self.mlp = MLP(config)
-        self.attn_scale = (1 / math.sqrt(2 * config.n_layer))
+        self.attn_scale = (1 / (2 * config.n_layer)**0.5)
 
     def forward(self, x):
         x = x + self.attn_scale * self.attn(rmsnorm(x))
@@ -114,7 +187,6 @@ class Block(nn.Module):
 
 @dataclass
 class GPTConfig:
-    block_size: int = 1024
     vocab_size: int = 50257
     n_layer: int = 12
     n_head: int = 12
@@ -135,7 +207,6 @@ class GPT(nn.Module):
 
     def forward(self, idx, targets=None, return_logits=True):
         b, t = idx.size()
-        assert t <= self.config.block_size, f"Cannot forward sequence of length {t}, block size is only {self.config.block_size}"
         pos = torch.arange(0, t, dtype=torch.long, device=idx.device) # shape (t)
 
         # forward the GPT model itself
@@ -148,10 +219,12 @@ class GPT(nn.Module):
         if targets is not None:
             # if we are given some desired targets also calculate the loss
             logits = self.lm_head(x)
+            logits = logits.float() # use tf32/fp32 for logits
             loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
         else:
             # inference-time mini-optimization: only forward the lm_head on the very last position
             logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
+            logits = logits.float() # use tf32/fp32 for logits
             loss = None
 
         # there are performance reasons why not returning logits is prudent, if not needed
@@ -160,8 +233,11 @@ class GPT(nn.Module):
 
         return logits, loss
 
-    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):
-        optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=betas)
+    def configure_optimizers(self, weight_decay, learning_rate, betas):
+        optimizer = CombinedOptimizer([
+            torch.optim.AdamW(self.lm_head.parameters(), lr=learning_rate, betas=betas, weight_decay=0),
+            OrthogonalNesterov(self.transformer.h.parameters(), lr=10 * learning_rate, momentum=0.95)
+        ])
         return optimizer
 
 # -----------------------------------------------------------------------------
@@ -210,9 +286,8 @@ class DistributedDataLoader:
         for fname in self.files:
             shard_ntok = _peek_data_shard(fname)
             assert shard_ntok >= num_processes * B * T + 1
-            ntok_total += shard_ntok
+            ntok_total += int(shard_ntok)
         self.ntok_total = ntok_total
-        print0(f"DataLoader: total number of tokens: {ntok_total:,} across {len(self.files)} files")
 
         # kick things off
         self.reset()
@@ -255,21 +330,23 @@ class Hyperparameters:
     input_bin = os.environ["NANOGPT_TRAIN_FILES"]
     input_val_bin = os.environ["NANOGPT_VAL_FILES"]
     val_tokens = int(os.environ["NANOGPT_VAL_TOKENS"]) # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
-
     model = "d12"
     # optimization
     batch_size = 64 # batch size in tokens
     sequence_length = 1024 # sequence length
-    total_batch_size = 524288
-    num_iterations = 9536 # number of iterations to run
-    val_loss_every = 128
-    weight_decay = 0.1
+    num_iterations = 7000 # number of iterations to run
+
     learning_rate = 0.0018
-    warmup_iters = 256
+    warmup_iters = 250
     warmdown_iters = 2000
-    save_every = 0
+    weight_decay = 0.1
+    grad_clip = 1.0
+    # evaluation
+    val_loss_every = 128 # every how many steps to evaluate val loss? 0 for only at the end
+    val_max_steps = 20 # how many batches of val to average?
+    save_every = 0 # every how many steps to save the checkpoint? 0 for only at the end
 
-    val_max_steps = 20
+    accumulation = 1
 
     output_dir = "pylog124m"
 
@@ -278,111 +355,125 @@ if __name__ == "__main__":
     import argparse
     print0(f"Running pytorch {torch.version.__version__}")
 
+    # parser = argparse.ArgumentParser()
+    # # file system input / output
+    # parser.add_argument("--input_bin", type=str, help="input .bin to train on")
+    # parser.add_argument("--input_val_bin", type=str, help="input .bin to eval validation loss on")
+    # parser.add_argument("--model", type=str, default="d12", help="d12|d24|d36|d48")
+    # # token layout for each step of the optimization
+    # parser.add_argument("--batch_size", type=int, default=4, help="batch size, in units of #batch dimensions")
+    # parser.add_argument("--accumulation", type=int, default=1)
+    # parser.add_argument("--sequence_length", type=int, default=64, help="sequence length")
+    # # workload (number of steps)
+    # parser.add_argument("--num_iterations", type=int, default=10, help="number of iterations to run")
+    # # optimization
+    # parser.add_argument("--learning_rate", type=float, default=1e-4, help="learning rate warmup iterations")
+    # parser.add_argument("--warmup_iters", type=int, default=0, help="learning rate warmup iterations")
+    # parser.add_argument("--warmdown_iters", type=int, default=0, help="learning rate warmdown iterations")
+    # parser.add_argument("--weight_decay", type=float, default=0.0, help="weight decay")
+    # # evaluation
+    # parser.add_argument("--val_loss_every", type=int, default=0, help="every how many steps to evaluate val loss?")
+    # parser.add_argument("--val_max_steps", type=int, default=20, help="how many batches of val to average?")
+    # parser.add_argument("--save_every", type=int, default=0, help="every how many steps to save the checkpoint")
+    # args = parser.parse_args()
     args = Hyperparameters()
 
     # args error checking and convenience variables
     B, T = args.batch_size, args.sequence_length
-    assert 1 <= T <= 1024
     assert args.model in {"d12", "d24", "d36", "d48"}
 
     # set up DDP (distributed data parallel). torchrun sets this env variable
-    # use of DDP atm demands CUDA, we set the device appropriately according to rank
-    assert torch.cuda.is_available(), "for now i think we need CUDA for DDP"
-    init_process_group(backend='nccl')
+    assert torch.cuda.is_available()
+    dist.init_process_group(backend='nccl')
     ddp_rank = int(os.environ['RANK'])
     ddp_local_rank = int(os.environ['LOCAL_RANK'])
     ddp_world_size = int(os.environ['WORLD_SIZE'])
     device = f'cuda:{ddp_local_rank}'
     torch.cuda.set_device(device)
-    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.
-    seed_offset = 0 # each process gets the exact same seed
     print(f"using device: {device}")
+    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.
 
-    tokens_per_fwdbwd = B * T * ddp_world_size
-    assert args.total_batch_size == tokens_per_fwdbwd
-
-    # set up a context manager following the desired dtype and device
-    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)
+    # load tokens
+    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
+    print0(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
+    val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
+    print0(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
+    x, y = train_loader.next_batch()
 
     # init the model from scratch
+    num_vocab = 50257
     model_config = {
-        "d12": GPTConfig(block_size=1024, vocab_size=50257, n_layer=12, n_head=12, n_embd=768),
-        "d24": GPTConfig(block_size=1024, vocab_size=50257, n_layer=24, n_head=16, n_embd=1024),
-        "d36": GPTConfig(block_size=1024, vocab_size=50257, n_layer=36, n_head=20, n_embd=1280),
-        "d48": GPTConfig(block_size=1024, vocab_size=50257, n_layer=48, n_head=25, n_embd=1600),
+        "d12": GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=12, n_embd=768),
+        "d24": GPTConfig(vocab_size=num_vocab, n_layer=24, n_head=16, n_embd=1024),
+        "d36": GPTConfig(vocab_size=num_vocab, n_layer=36, n_head=20, n_embd=1280),
+        "d48": GPTConfig(vocab_size=num_vocab, n_layer=48, n_head=25, n_embd=1600),
     }[args.model]
     model = GPT(model_config)
-    model = model.train().cuda()
+    model = model.cuda()
     if hasattr(config, "coordinate_descent_tuning"):
         config.coordinate_descent_tuning = True # suggested by @Chillee
     print0("compiling the model...")
     model = torch.compile(model)
 
-    # load tokens
-    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
-    val_loader = None
-    if args.input_val_bin:
-        val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
-    x, y = train_loader.next_batch()
-
     # here we wrap model into DDP container
     model = DDP(model, device_ids=[ddp_local_rank])
     raw_model = model.module # always contains the "raw" unwrapped model
 
+    # set up a context manager following the desired dtype and device
+    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)
+
     # init the optimizer
     optimizer = raw_model.configure_optimizers(weight_decay=args.weight_decay,
-                                               learning_rate=args.learning_rate, betas=(0.9, 0.95),
-                                               device_type=device)
+                                               learning_rate=args.learning_rate, betas=(0.9, 0.95))
 
     # learning rate decay scheduler (linear warmup and warmdown)
     def get_lr(it):
         assert it <= args.num_iterations
         # 1) linear warmup for warmup_iters steps
         if it < args.warmup_iters:
-            return args.learning_rate * (it+1) / args.warmup_iters
+            return (it+1) / args.warmup_iters
         # 2) constant lr for a while
         elif it < args.num_iterations - args.warmdown_iters:
-            return args.learning_rate
+            return 1.0
         # 3) linear warmdown
         else:
             decay_ratio = (args.num_iterations - it) / args.warmdown_iters
-            return args.learning_rate * decay_ratio
+            return decay_ratio
 
     run_id = str(uuid.uuid4())
-
-    # create the logging directory if it does not exist
-    logfile = None
-    if master_process and args.output_dir:
-        os.makedirs(args.output_dir, exist_ok=True)
-        logfile = os.path.join(args.output_dir, "%s.log" % run_id)
-        # create the log file "main.log" inside it, and wipe it clean
+    if master_process:
+        os.makedirs('logs/%s' % run_id, exist_ok=True)
+        logfile = 'logs/%s/log.txt' % run_id
+        # create the empty log file
         with open(logfile, "w") as f:
             pass
 
-    timings = []
     for step in range(args.num_iterations + 1):
-        t0 = time.time()
         last_step = (step == args.num_iterations)
 
         # once in a while evaluate the validation dataset
-        if (args.val_loss_every > 0 \
-            and (step % args.val_loss_every == 0 or last_step)) \
-            and (val_loader is not None):
+        if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
             model.eval()
             val_loader.reset()
-            with torch.no_grad():
-                val_loss = 0.0
-                for _ in range(args.val_max_steps):
+            val_loss = 0.0
+            for _ in range(args.val_max_steps):
+                with torch.no_grad(): # I want to use ctx here but it causes a torch.compile error
                     x_val, y_val = val_loader.next_batch()
                     _, loss = model(x_val, y_val, return_logits=False)
-                    val_loss += loss.item()
-                val_loss /= args.val_max_steps
-            # log to console and to file
+                    val_loss += loss
+            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
+            val_loss /= args.val_max_steps
+            # log val loss to console and to logfile
             print0(f"val loss {val_loss}")
             if master_process and logfile is not None:
                 with open(logfile, "a") as f:
                     f.write("s:%d tel:%f\n" % (step, val_loss))
 
+        # save the state of the training process
+        if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
+            log = dict(step=step, args=args.__dict__, code=code, model=raw_model.state_dict(), optimizer=optimizer.state_dict())
+            torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
+
         # bit confusing: we want to make sure to eval on 0th iteration
         # but also after the very last iteration. so we loop for step <= num_iterations
         # instead of just < num_iterations (one extra due to <=), only to do
@@ -390,60 +481,42 @@ if __name__ == "__main__":
         if last_step:
             break
 
+        torch.cuda.synchronize()
+        t0 = time.time()
         # --------------- TRAINING SECTION BEGIN -----------------
         model.train()
-        # forward pass
-        with ctx:
-            _, loss = model(x, y, return_logits=False)
-        # advance the dataset for the next batch
-        x, y = train_loader.next_batch()
-        # backward pass
-        loss.backward()
+        for _ in range(args.accumulation):
+            # forward pass
+            with ctx:
+                _, loss = model(x, y, return_logits=False)
+                train_loss = loss.detach()
+            # advance the dataset for the next batch
+            x, y = train_loader.next_batch()
+            # backward pass
+            loss.backward()
         for p in model.parameters():
-            p.grad = p.grad / (p.grad.norm() + 1e-6)
+            p.grad /= args.accumulation
         # determine and set the learning rate for this iteration
-        lr = get_lr(step)
-        for param_group in optimizer.param_groups:
-            param_group['lr'] = lr
+        lr_scale = get_lr(step)
+        optimizer.scale_lrs(lr_scale)
         # step the optimizer
         optimizer.step()
         optimizer.zero_grad(set_to_none=True)
         # --------------- TRAINING SECTION END -------------------
         # everything that follows now is just diagnostics, prints, logging, etc.
-
         torch.cuda.synchronize()
-        # time and print
         t1 = time.time()
-        # the 0th iteration is often an outlier (much slower) => skip logging it
-        tokens_per_second = ddp_world_size * B * T / (t1-t0)
-        lossf = loss.item() # keep track of the mean loss
-        print0(f"step {step+1:4d}/{args.num_iterations} | train loss {lossf:.6f} | lr {lr:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)")
-        # log to logile
-        if master_process and logfile is not None:
-            with open(logfile, "a") as f:
-                f.write("s:%d trl:%f\n" % (step, lossf))
-
-        # keep track of smooth timings, last 20 iterations
-        if step > 0 and step > args.num_iterations - 20:
-            timings.append(t1-t0)
 
-        if master_process and (args.save_every > 0 and step % args.save_every == 0):
-            log = dict(model=raw_model.state_dict(), code=code, args=args.__dict__)
-            os.makedirs('logs/%s' % run_id, exist_ok=True)
-            torch.save(log, 'logs/%s/model_step%06d.pt' % (run_id, step))
+        dist.all_reduce(train_loss, op=dist.ReduceOp.AVG)
+        tokens_per_second = ddp_world_size * B * T / (t1 - t0)
+        print0(f"step {step+1:4d}/{args.num_iterations} | train loss {train_loss.item():.4f} | lr_scale {lr_scale:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)")
+        # log training loss to logfile
+        if master_process:
+            with open(logfile, "a") as f:
+                f.write("s:%d trl:%f\n" % (step, train_loss.item()))
 
-    # print the average of the last 20 timings, to get something smooth-ish
-    timings = timings[-20:]
-    print0(f"final {len(timings)} iters avg: {np.mean(timings)*1000:.3f}ms")
     print0(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
 
-    # -------------------------------------------------------------------------
-
-    if master_process:
-        log = dict(model=raw_model.state_dict(), code=code, args=args.__dict__)
-        os.makedirs('logs/%s' % run_id, exist_ok=True)
-        torch.save(log, 'logs/%s/final.pt' % run_id)
-
     # -------------------------------------------------------------------------
     # clean up nice
-    destroy_process_group()
\ No newline at end of file
+    dist.destroy_process_group()
\ No newline at end of file
