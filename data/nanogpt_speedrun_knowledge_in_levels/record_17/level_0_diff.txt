diff --git a/temp_current.py b/temp_next.py
index 2e04c88..3560bc8 100644
--- a/temp_current.py
+++ b/temp_next.py
@@ -1,30 +1,28 @@
 import os
 import sys
-import glob
 with open(sys.argv[0]) as f:
     code = f.read() # read the code of this file ASAP, for logging
-os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
-import contextlib
-import time
 import uuid
+import time
+import glob
+import subprocess
+import contextlib
 from dataclasses import dataclass
-from pathlib import Path
 
 import torch
-import torch._inductor.config as config
-import torch.distributed as dist
+torch.empty(1, device='cuda', requires_grad=True).backward()
+from torch import nn
 import torch.nn.functional as F
-from torch import Tensor, nn
-
-# Use of FlexAttention contributed by @KoszarskyB
-from torch.nn.attention.flex_attention import BlockMask, flex_attention
+import torch.distributed as dist
 from torch.nn.parallel import DistributedDataParallel as DDP
+# use of FlexAttention contributed by @KoszarskyB
+from torch.nn.attention.flex_attention import BlockMask, flex_attention
 
 # -----------------------------------------------------------------------------
 # Muon optimizer
 
 @torch.compile
-def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7) -> Tensor:
+def zeropower_via_newtonschulz5(G, steps):
     """
     Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
     quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
@@ -37,13 +35,17 @@ def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7) -> Tensor:
     assert len(G.shape) == 2
     a, b, c = (3.4445, -4.7750,  2.0315)
     X = G.bfloat16()
-    X /= (X.norm() + eps) # ensure top singular value <= 1
     if G.size(0) > G.size(1):
         X = X.T
+
+    # Ensure spectral norm is at most 1
+    X = X / (X.norm() + 1e-7)
+    # Perform the NS iterations
     for _ in range(steps):
         A = X @ X.T
         B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
         X = a * X + B @ X
+    
     if G.size(0) > G.size(1):
         X = X.T
     return X
@@ -73,31 +75,27 @@ class Muon(torch.optim.Optimizer):
         ns_steps: The number of Newton-Schulz iteration steps to use.
     """
     def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
+        self.world_size = int(os.environ['WORLD_SIZE'])
+        self.rank = int(os.environ['RANK'])
         defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
-        params: "list[Tensor]" = [*params]
-        assert all(isinstance(p, Tensor) for p in params)
+        assert all(isinstance(p, torch.Tensor) for p in params)
         sizes = {p.numel() for p in params}
-        param_groups = [
-            {
-                'params': [p for p in params if p.numel() == size],
-                'update_buffer': [
-                    torch.empty(size, device='cuda', dtype=torch.bfloat16)
-                    for _ in range(world_size)
-                ],
-            }
-            for size in sizes
-        ]
+        param_groups = [dict(params=[p for p in params if p.numel() == size],
+                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
+                        for size in sizes]
         super().__init__(param_groups, defaults)
 
     def step(self):
+
         for group in self.param_groups:
+
             lr = group['lr']
             momentum = group['momentum']
             nesterov = group['nesterov']
             ns_steps = group['ns_steps']
-            update_buffers: "list[Tensor]" = group['update_buffer']
+            update_buffers = group['update_buffer']
             # generate weight updates in distributed fashion
-            params: "list[Tensor]" = group['params']
+            params = group['params']
             handle = None
             params_world = None
             def update_prev():
@@ -106,28 +104,27 @@ class Muon(torch.optim.Optimizer):
                 assert handle is not None
                 handle.wait()
                 for p_world, g_world in zip(params_world, update_buffers):
-                    param_lr = getattr(p_world, "lr", 1.0)
                     p_world.data.add_(
                         g_world.view_as(p_world),
-                        alpha=-lr * param_lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
+                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                     )
-            for base_i in range(len(params))[::world_size]:
+            for base_i in range(len(params))[::self.world_size]:
                 if base_i + rank < len(params):
-                    p = params[base_i + rank]
+                    p = params[base_i + self.rank]
                     g = p.grad
                     assert g is not None
                     state = self.state[p]
                     if 'momentum_buffer' not in state:
                         state['momentum_buffer'] = torch.zeros_like(g)
-                    buf: Tensor = state['momentum_buffer']
+                    buf = state['momentum_buffer']
                     buf.lerp_(g, 1 - momentum)
                     g = g.lerp_(buf, momentum) if nesterov else buf
                     g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                 else:
                     g = update_buffers[rank]
-                update_prev()
+                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                 handle = dist.all_gather(update_buffers, g, async_op=True)
-                params_world = params[base_i : base_i + world_size]
+                params_world = params[base_i : base_i + self.world_size]
             update_prev()
 
 # -----------------------------------------------------------------------------
@@ -137,6 +134,7 @@ def norm(x):
     return F.rms_norm(x, (x.size(-1),))
 
 class CastedLinear(nn.Linear):
+
     def __init__(self, in_features, out_features):
         super().__init__(in_features, out_features, bias=False)
 
@@ -144,25 +142,27 @@ class CastedLinear(nn.Linear):
         return F.linear(x, self.weight.type_as(x))
 
 class Rotary(nn.Module):
+
     def __init__(self, dim, max_seq_len=65536):
         super().__init__()
-        inv_freq = (1 / 1024) ** torch.linspace(0.0, 1.0, steps=dim // 4, dtype=torch.float32)
-        inv_freq = torch.cat([inv_freq, inv_freq.new_zeros(dim // 4)])
+        # half-truncate RoPE by @YouJiacheng
+        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
+        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
         t = torch.arange(max_seq_len, dtype=torch.float32)
-        theta = torch.einsum("i, j -> ij", t, inv_freq)
+        theta = torch.einsum('i,j -> ij', t, angular_freq)
         self.cos = nn.Buffer(theta.cos(), persistent=False)
         self.sin = nn.Buffer(theta.sin(), persistent=False)
 
-    def forward(self, x: Tensor):
+    def forward(self, x):
         cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
-        x1, x2 = x.to(dtype=torch.float32).chunk(2, dim=-1)
+        x1, x2 = x.float().chunk(2, dim=-1)
         y1 = x1 * cos + x2 * sin
         y2 = x1 * (-sin) + x2 * cos
         return torch.cat((y1, y2), 3).type_as(x)
 
 class CausalSelfAttention(nn.Module):
 
-    def __init__(self, dim: int, num_heads: int):
+    def __init__(self, dim, num_heads):
         super().__init__()
         assert dim % num_heads == 0
         self.num_heads = num_heads
@@ -174,16 +174,16 @@ class CausalSelfAttention(nn.Module):
         self.c_proj = CastedLinear(dim, dim)
         self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
 
-    def forward(self, x: Tensor, vi: Tensor | None, block_mask: BlockMask):
+    def forward(self, x, ve, block_mask):
         B, T = x.size(0), x.size(1) # batch size, sequence length
-        assert B == 1, "Must use batch size = 1 for FlexAttention"
+        assert B == 1, 'Must use batch size = 1 for FlexAttention'
         q = self.c_q(x).view(B, T, self.num_heads, -1)
         k = self.c_k(x).view(B, T, self.num_heads, -1)
         v = self.c_v(x).view(B, T, self.num_heads, -1)
-        if vi is None:
+        if ve is not None:
+            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
+        else: # skip mid-layers token value embeddings by @YouJiacheng
             v = self.lambdas[0] * v
-        else:
-            v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @KoszarskyB & @Grad62304977
         q, k = norm(q), norm(k) # QK norm @Grad62304977
         q, k = self.rotary(q), self.rotary(k)
         y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
@@ -195,7 +195,7 @@ class MLP(nn.Module):
 
     def __init__(self, dim):
         super().__init__()
-        self.c_fc   = CastedLinear(dim, 4 * dim)
+        self.c_fc = CastedLinear(dim, 4 * dim)
         self.c_proj = CastedLinear(4 * dim, dim)
         self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
 
@@ -207,83 +207,57 @@ class MLP(nn.Module):
 
 class Block(nn.Module):
 
-    def __init__(self, config: "GPTConfig", layer_idx: int):
+    def __init__(self, model_dim, num_heads, use_attn=True):
         super().__init__()
-        if layer_idx != 7:
-            self.attn = CausalSelfAttention(config.model_dim, config.num_heads)
-        self.mlp = MLP(config.model_dim)
+        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
+        self.mlp = MLP(model_dim)
         self.lambdas = nn.Parameter(torch.tensor([1., 0.]))
-        self.layer_idx = layer_idx
 
-    def forward(self, x, vi, x0, block_mask):
+    def forward(self, x, ve, x0, block_mask):
         x = self.lambdas[0] * x + self.lambdas[1] * x0
-        if self.layer_idx != 7:
-            x = x + self.attn(norm(x), vi, block_mask)
+        if self.attn is not None:
+            x = x + self.attn(norm(x), ve, block_mask)
         x = x + self.mlp(norm(x))
         return x
 
 class ValueEmbedding(nn.Module):
-    def __init__(self, config: "GPTConfig"):
+    def __init__(self, vocab_size, model_dim):
         super().__init__()
-        self.__setattr__
-        self.embed = nn.ModuleList([
-            nn.Embedding(config.vocab_size, config.model_dim)
-            for _ in range(3)
-        ])
-
-    def forward(self, inputs) -> "list[Tensor | None]":
-        ve = [emb(inputs) for emb in self.embed]
-        ve = [
-            ve[0], ve[1], ve[2],
-            None, None, None,
-            None, None, None,
-            ve[0], ve[1], ve[2],
-        ]
-        return ve
+        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
 
+    def forward(self, inputs):
+        ve = [emb(inputs).bfloat16() for emb in self.embed]
+        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
+        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
+        return ve
 
 # -----------------------------------------------------------------------------
 # The main GPT-2 model
 
-@dataclass
-class GPTConfig:
-    vocab_size : int = 50257
-    num_layers : int = 12
-    num_heads : int = 6 # head dim 128 suggested by @Grad62304977
-    model_dim : int = 768
-    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
-    # this originates from Karpathy's experiments.
-    def vocab_size_next_multiple_of(self, n: int):
-        v = self.vocab_size
-        return next(x for x in range(v + n)[::n] if x >= v)
-
 class GPT(nn.Module):
 
-    def __init__(self, config: GPTConfig):
+    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
         super().__init__()
-        self.num_layers = config.num_layers
-
-        # U-net design by @brendanh0gan
-        self.num_encoder_layers = config.num_layers // 2 # Half of the layers for encoder
-        self.num_decoder_layers = config.num_layers - self.num_encoder_layers # Remaining for decoder
-        # Add learnable skip connection weights for decoder layers
-        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
-
-        self.embed = nn.Embedding(config.vocab_size, config.model_dim)
-        self.blocks = nn.ModuleList([Block(config, layer_idx) for layer_idx in range(config.num_layers)])
+        self.embed = nn.Embedding(vocab_size, model_dim)
+        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
+        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
+                                     for i in range(num_layers)])
         # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
         # U-net structure on token value embeddings by @leloykun
-        self.value_embeds = ValueEmbedding(config)
-        self.lm_head = CastedLinear(config.model_dim, config.vocab_size_next_multiple_of(128))
+        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
+        self.lm_head = CastedLinear(model_dim, vocab_size)
         self.lm_head.weight.data.zero_() # @Grad62304977
+        # U-net design by @brendanh0gan
+        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
+        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
+        # Add learnable skip connection weights for decoder layers
+        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
 
-    def forward(
-        self,
-        inputs: Tensor,
-        targets: Tensor,
-        sliding_window_num_blocks: Tensor,
-    ):
+    def forward(self, inputs, targets, sliding_window_num_blocks):
         BLOCK_SIZE = 128
+        seq_len = len(inputs)
+        assert seq_len % BLOCK_SIZE == 0
+        total_num_blocks = seq_len // BLOCK_SIZE
         assert inputs.ndim == 1
         docs = (inputs == 50256).cumsum(0)
         docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
@@ -293,19 +267,19 @@ class GPT(nn.Module):
             causal_mask = q_idx >= kv_idx
             document_mask = docs[q_idx] == docs[kv_idx]
             return causal_mask & document_mask
-        
-        def dense_to_ordered(dense_mask: Tensor):
+
+        def dense_to_ordered(dense_mask):
             num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
             indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
             return num_blocks[None, None].contiguous(), indices[None, None].contiguous()
 
-        def create_doc_swc_block_mask(sliding_window_num_blocks: Tensor):
-            kv_idx = block_idx = torch.arange(512, dtype=torch.int32, device="cuda")
+        def create_doc_swc_block_mask(sliding_window_num_blocks):
+            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
             q_idx = block_idx[:, None]
             causal_bm = q_idx >= kv_idx
             causal_full_bm = q_idx > kv_idx
             window_bm = q_idx - kv_idx < sliding_window_num_blocks
-            window_full_bm = window_bm
+            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
             # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
             document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
             document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
@@ -324,11 +298,10 @@ class GPT(nn.Module):
 
         block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)
 
-        # forward the GPT model itself
-        x = self.embed(inputs[None]) # token embeddings of shape (b, t, model_dim)
-        x = norm(x) # @Grad62304977
-        x0 = x
+        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
+        x = x0
         ve = self.value_embeds(inputs)
+        assert len(ve) == len(self.blocks)
         ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
 
         # Store outputs for U-Net skip connections
@@ -345,68 +318,57 @@ class GPT(nn.Module):
 
         x = norm(x)
         logits = self.lm_head(x)
-        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
+        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
         logits = logits.float()
-        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
+        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
         return loss
 
 # -----------------------------------------------------------------------------
 # Our own simple Distributed Data Loader
 
-def _peek_data_shard(file: Path):
+def _load_data_shard(path):
     # only reads the header, returns header data
     # header is 256 int32
-    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32)
-    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
-    assert header[1] == 1, "unsupported version"
-    return int(header[2]) # number of tokens (claimed)
-
-def _load_data_shard(path: Path, num_tokens):
-    # with path.open("rb", buffering=0) as f:
-    with open(path, "rb", buffering=0) as f:
-        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)
+    header = torch.from_file(path, False, 256, dtype=torch.int32)
+    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
+    assert header[1] == 1, 'unsupported version'
+    num_tokens = int(header[2]) # number of tokens (claimed)
+    with open(path, 'rb', buffering=0) as f:
+        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
         f.seek(256 * 4)
-        nbytes = f.readinto(tokens.numpy())
-        assert nbytes == 2 * num_tokens, "number of tokens read does not match header?"
+        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
+        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
     return tokens
 
 class DistributedDataLoader:
-    def __init__(self, filename_pattern, seq_len, process_rank, num_processes):
-        self.process_rank = process_rank
-        self.num_processes = num_processes
-        self.seq_len = seq_len
 
-        # glob files that match the pattern
+    def __init__(self, filename_pattern):
+        self.rank = int(os.environ['RANK'])
+        self.world_size = int(os.environ['WORLD_SIZE'])
         self.files = sorted(glob.glob(filename_pattern))
-        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"
-
-        # load and validate all data shards, count number of tokens in total
-        self.files_num_tokens = [_peek_data_shard(file) for file in self.files]
-        assert min(self.files_num_tokens) >= num_processes * seq_len + 1
-        self.total_num_tokens = sum(self.files_num_tokens)
-
         self.reset()
 
     def reset(self):
         self.current_shard = -1
         self.advance()
 
-    def advance(self): # advance to next data shard
+    def advance(self):
         self.current_shard = (self.current_shard + 1) % len(self.files)
-        self.current_position = self.process_rank * self.seq_len
-        self.tokens = _load_data_shard(self.files[self.current_shard], self.files_num_tokens[self.current_shard])
-
-    def next_batch(self):
-        batch_size = self.seq_len * self.num_processes
-        buf = self.tokens[self.current_position:self.current_position+self.seq_len+1]
-        # host side async is sufficient;
-        # no performance improvement was observed when introducing a separate stream.
-        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
-        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # targets
-        # advance current position and load next shard if necessary
-        self.current_position += batch_size
+        self.current_position = 0
+        self.tokens = _load_data_shard(self.files[self.current_shard])
+
+    def next_batch(self, batch_size):
+        assert batch_size % self.world_size == 0
+        device_batch_size = batch_size // self.world_size
+        # load next shard if necessary
         if self.current_position + batch_size + 1 >= len(self.tokens):
             self.advance()
+        pos = self.current_position + self.rank * device_batch_size
+        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
+        # advance current position
+        self.current_position += batch_size
+        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
+        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
         return inputs, targets
 
 # -----------------------------------------------------------------------------
@@ -414,32 +376,31 @@ class DistributedDataLoader:
 
 @dataclass
 class Hyperparameters:
-    # data hyperparams
-    input_bin = os.environ["NANOGPT_TRAIN_FILES"]
-    input_val_bin = os.environ["NANOGPT_VAL_FILES"]
+    # data
+    train_bin = os.environ["NANOGPT_TRAIN_FILES"]
+    val_bin = os.environ["NANOGPT_VAL_FILES"]
     val_tokens = int(os.environ["NANOGPT_VAL_TOKENS"]) # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
-    # optimization hyperparams
-    batch_size : int = 8 # batch size, in sequences, across all devices
-    sequence_length : int = 64*1024 # sequence length, in tokens
-    num_iterations : int = 1490 # number of iterations to run
-    warmup_iters : int = 0
-    cooldown_iters : int = 600 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
-    weight_decay : float = 0
-    # evaluation and logging hyperparams
-    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
-    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
-    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
+    # optimization
+    batch_size = 8*64*1024 # batch size in tokens
+    max_device_batch_size = 64*1024 # batch size per device in tokens
+    num_iterations = 1390 # number of iterations to run
+    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
+    bf16_embeds = True
+    # evaluation and logging
+    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
+    # implementation
+    save_checkpoint = False
 args = Hyperparameters()
 
+micro_bs = args.max_device_batch_size
+
 # set up DDP (distributed data parallel). torchrun sets this env variable
 rank = int(os.environ['RANK'])
 local_rank = int(os.environ['LOCAL_RANK'])
 world_size = int(os.environ['WORLD_SIZE'])
 assert torch.cuda.is_available()
-device = torch.device(f"cuda:{local_rank}")
-torch.cuda.set_device(device)
-print(f"using device: {device}")
-dist.init_process_group(backend='nccl', device_id=device)
+torch.cuda.set_device(local_rank)
+dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
 dist.barrier()
 master_process = (rank == 0) # this process will do logging, checkpointing etc.
 
@@ -447,96 +408,86 @@ master_process = (rank == 0) # this process will do logging, checkpointing etc.
 logfile = None
 if master_process:
     run_id = uuid.uuid4()
-    os.makedirs("logs", exist_ok=True)
-    logdir = Path("logs") / f"{run_id}"
-    logdir.mkdir(parents=True, exist_ok=True)
-    logfile = Path("logs") / f"{run_id}.txt"
-    print(logfile.stem)
-    # create the log file
-    with logfile.open("w") as f:
-        # begin the log by printing this file (the Python code)
-        print(code, file=f)
-        print("=" * 100, file=f)
-def print0(s, logonly=False):
+    os.makedirs('logs', exist_ok=True)
+    logfile = f'logs/{run_id}.txt'
+    print(logfile)
+
+def print0(s, console=True):
     if master_process:
-        with logfile.open("a") as f:
-            if not logonly:
+        with open(logfile, 'a') as f:
+            if console:
                 print(s)
             print(s, file=f)
-# log information about the hardware/software environment this is running on
-# and print the full `nvidia-smi` to file
-print0(f"Running python {sys.version}")
-print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
-import subprocess
 
-result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-print0(f'{result.stdout}', logonly=True)
-print0('='*100, logonly=True)
-
-# calculate the number of steps to take in the val loop.
-assert args.val_tokens % (args.sequence_length * world_size) == 0
-val_steps = args.val_tokens // (args.sequence_length * world_size)
-# calculate the steps of gradient accumulation required to attain the desired global batch size.
-assert args.batch_size % (world_size) == 0
-train_accumulation_steps = args.batch_size // world_size
-
-# load tokens
-train_loader = DistributedDataLoader(args.input_bin, args.sequence_length, rank, world_size)
-val_loader = DistributedDataLoader(args.input_val_bin, args.sequence_length, rank, world_size)
-print0(f"Training DataLoader: total number of tokens: {train_loader.total_num_tokens} across {len(train_loader.files)} files")
-print0(f"Validation DataLoader: total number of tokens: {val_loader.total_num_tokens} across {len(val_loader.files)} files")
-print0('='*100, logonly=True)
-inputs_train, targets_train = train_loader.next_batch()
-
-model = GPT(GPTConfig(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768))
-model = model.cuda().bfloat16()
-for m in model.modules():
-    if isinstance(m, CastedLinear):
-        m.float()
-config.coordinate_descent_tuning = True # suggested by @Chillee
-# config.max_autotune = True
-# config.cpp_wrapper = True
-model: nn.Module = torch.compile(model)
-# here we wrap model into DDP container
-model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)
-raw_model = model.module # always contains the "raw" unwrapped model
-assert isinstance(raw_model, nn.Module)
+# begin by printing this file (the Python code)
+print0(code)
+print0('='*100)
+# log information about the hardware/software environment this is running on
+print0(f'Running Python {sys.version}')
+print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
+print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
+print0('='*100)
+
+# load data
+train_loader = DistributedDataLoader(args.train_bin)
+val_loader = DistributedDataLoader(args.val_bin)
+print0(f'Training dataloader files: {train_loader.files}')
+print0(f'Validation dataloader files: {val_loader.files}')
+print0('='*100)
+
+# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
+# this originates from Karpathy's experiments.
+model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
+model = model.cuda()
+if args.bf16_embeds:
+    for m in model.modules():
+        if isinstance(m, nn.Embedding):
+            m.bfloat16()
+model = torch.compile(model)
+ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)
+
+# collect the parameters to optimize
+hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
+embed_params = [model.embed.weight, *model.value_embeds.parameters()]
+scalar_params = [p for p in model.parameters() if p.ndim < 2]
+head_params = [model.lm_head.weight]
 
 # init the optimizer(s)
-embed_params = [*raw_model.embed.parameters(), *raw_model.value_embeds.parameters()]
-optimizer1 = torch.optim.Adam(embed_params, lr=0.6, betas=(0.8, 0.95), fused=True)
-optimizer2 = torch.optim.Adam([raw_model.lm_head.weight], lr=0.008, betas=(0.8, 0.95), fused=True)
-params = list(raw_model.blocks.parameters())
-matrix_params = [p for p in params if p.ndim == 2]
-scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
-optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
-optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
-optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
-# learning rate decay scheduler (linear warmup and cooldown)
+optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
+                               dict(params=head_params, lr=0.008),
+                               dict(params=scalar_params, lr=0.04)],
+                              betas=(0.8, 0.95), fused=True)
+optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
+optimizers = [optimizer1, optimizer2]
+
+# learning rate schedule: stable then decay
 def get_lr(it):
-    assert it <= args.num_iterations
-    # 1) linear warmup for warmup_iters steps
-    if it < args.warmup_iters:
-        return (it+1) / args.warmup_iters
-    # 2) constant lr for a while
-    elif it < args.num_iterations - args.cooldown_iters:
+    t = 1 - it / args.num_iterations # time remaining in training
+    assert 1 >= t > 0
+    # 1) constant lr for first part of training
+    if t >= args.cooldown_frac:
         return 1.0
-    # 3) linear cooldown
+    # 2) then linear cooldown
     else:
-        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
-        return decay_ratio
+        return t / args.cooldown_frac
 schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
 
-sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device="cuda")
-sw_num_blocks_prev = 1
+# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
+def get_sliding_window_blocks(it):
+    x = it / args.num_iterations # training progress
+    assert 0 <= x <= 1
+    return int(((1 - x) * 128 + x * 1856) // 128)
+sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')
+
 # Start training loop
 training_time_ms = 0
 # start the clock
 torch.cuda.synchronize()
 t0 = time.perf_counter()
 # begin training
-for step in range(args.num_iterations + 1):
-    last_step = (step == args.num_iterations)
+train_steps = args.num_iterations
+for step in range(train_steps + 1):
+    last_step = (step == train_steps)
     # This effectively ignores timing first 10 steps, which are slower for weird reasons.
     # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
     # steps with dummy data first, and then re-initialize the model and reset the loader.
@@ -545,15 +496,10 @@ for step in range(args.num_iterations + 1):
         t0 = time.perf_counter()
     timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val
 
-    # Linearly increase the sliding window size over training in chunks of 64 from 64 -> 1792. By @fernbear.bsky.social
-    frac_done = step / args.num_iterations # training progress
-    sw_num_blocks = int(((1 - frac_done) * 64 + frac_done * 1792 + 64) // 128)
-    if sw_num_blocks != sw_num_blocks_prev:
-        sliding_window_num_blocks.copy_(sw_num_blocks, non_blocking=True)
-        sw_num_blocks_prev = sw_num_blocks
+    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))
 
-    # once in a while evaluate the validation dataset
-    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
+    # --------------- VALIDATION SECTION -----------------
+    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
         # stop the clock
         torch.cuda.synchronize()
         training_time_ms += 1000 * (time.perf_counter() - t0)
@@ -561,71 +507,52 @@ for step in range(args.num_iterations + 1):
         model.eval()
         val_loader.reset()
         val_loss = 0.0
+        # calculate the number of steps to take in the val loop.
+        val_batch_size = world_size * micro_bs
+        assert args.val_tokens % val_batch_size == 0
+        val_steps = args.val_tokens // val_batch_size
         for _ in range(val_steps):
             with torch.no_grad():
-                inputs_val, targets_val = val_loader.next_batch()
-                val_loss += model(inputs_val, targets_val, sliding_window_num_blocks)
+                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
+                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
         dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
         val_loss /= val_steps
-        # log val loss to console and to logfile
-        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
+        # logging
+        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
         # start the clock again
         torch.cuda.synchronize()
         t0 = time.perf_counter()
 
-    # if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
-    #     # stop the clock
-    #     torch.cuda.synchronize()
-    #     training_time_ms += 1000 * (time.perf_counter() - t0)
-    #     # save the state of the training process
-    #     log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
-    #     torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
-    #     # start the clock again
-    #     torch.cuda.synchronize()
-    #     t0 = time.perf_counter()
-
-    # bit confusing: we want to make sure to eval on 0th iteration
-    # but also after the very last iteration. so we loop for step <= num_iterations
-    # instead of just < num_iterations (one extra due to <=), only to do
-    # the validation/sampling one last time, and then we break right here as we're done.
     if last_step:
+        if master_process and args.save_checkpoint:
+            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
+            os.makedirs(f'logs/{run_id}', exist_ok=True)
+            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
+        # the last step only has the validation loop, so break to avoid training
         break
 
-    # --------------- TRAINING SECTION BEGIN -----------------
+    # --------------- TRAINING SECTION -----------------
     model.train()
-    for i in range(1, train_accumulation_steps + 1):
-        with contextlib.ExitStack() as stack:
-            if i < train_accumulation_steps: # there's no need to sync gradients every accumulation step
-                stack.enter_context(model.no_sync())
-            if step >= 5:
-                stack.enter_context(torch.compiler.set_stance(skip_guard_eval_unsafe=True))
-            loss = model(inputs_train, targets_train, sliding_window_num_blocks)
-            loss.backward()
-            del loss
-            inputs_train, targets_train = train_loader.next_batch()
-    if train_accumulation_steps != 1:
-        for p in model.parameters():
-            p.grad /= train_accumulation_steps
+    batch_size = args.batch_size
+    assert batch_size % world_size == 0
+    inputs_train, targets_train = train_loader.next_batch(batch_size)
+    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
+    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
+        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
     # momentum warmup for Muon
     frac = min(step/300, 1)
-    for group in optimizer3.param_groups:
+    for group in optimizer2.param_groups:
         group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
     # step the optimizers and schedulers
     for opt, sched in zip(optimizers, schedulers):
         opt.step()
-        sched.step()
+        if step != train_steps-1:
+            sched.step()
     # null the gradients
     model.zero_grad(set_to_none=True)
-    # --------------- TRAINING SECTION END -------------------
-    # everything that follows now is just diagnostics, prints, logging, etc.
+    # logging
     approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
-    print0(f"step:{step+1}/{args.num_iterations} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
-
-print0(
-    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
-    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
-)
+    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)
 
-# -------------------------------------------------------------------------
-# clean up nice
+print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
 dist.destroy_process_group()
