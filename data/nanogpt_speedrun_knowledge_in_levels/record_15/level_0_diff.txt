diff --git a/temp_current.py b/temp_next.py
index 9d6b788..2baac77 100644
--- a/temp_current.py
+++ b/temp_next.py
@@ -5,6 +5,7 @@ with open(sys.argv[0]) as f:
     code = f.read() # read the code of this file ASAP, for logging
 import uuid
 import time
+import contextlib
 from dataclasses import dataclass
 from pathlib import Path
 
@@ -14,16 +15,11 @@ import torch.nn.functional as F
 import torch.distributed as dist
 import torch._inductor.config as config
 from torch.nn.parallel import DistributedDataParallel as DDP
-# Use of FlexAttention contributed by @KoszarskyB
-from torch.nn.attention.flex_attention import BlockMask, flex_attention
+from torch.nn.attention.flex_attention import BlockMask, flex_attention #KoszarskyB
 
 # -----------------------------------------------------------------------------
 # Muon optimizer
 
-def zeropower_via_svd(G, steps=None):
-    U, S, V = G.svd()
-    return U @ V.T
-
 @torch.compile
 def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
     """
@@ -49,8 +45,6 @@ def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
         X = X.T
     return X
 
-zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)
-
 class Muon(torch.optim.Optimizer):
     """
     Muon - MomentUm Orthogonalized by Newton-schulz
@@ -73,23 +67,21 @@ class Muon(torch.optim.Optimizer):
         lr: The learning rate used by the internal SGD.
         momentum: The momentum used by the internal SGD.
         nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
-        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
-        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
+        ns_steps: The number of Newton-Schulz iteration steps to use.
     """
-    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
-                 backend='newtonschulz5', backend_steps=5):
-        self.num_process = int(os.environ['WORLD_SIZE'])
-        self.rank = int(os.environ["RANK"])
-        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
-        params: "list[torch.Tensor]" = list(params)
+    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
+        self.world_size = int(os.environ['WORLD_SIZE'])
+        self.rank = int(os.environ['RANK'])
+        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
+        params = list(params)
         assert all(isinstance(p, torch.Tensor) for p in params)
         sizes = {p.numel() for p in params}
         param_groups = [
             {
-                "params": [p for p in params if p.numel() == size],
-                "update_buffer": [
-                    torch.empty(size, device="cuda", dtype=torch.bfloat16)
-                    for _ in range(self.num_process)
+                'params': [p for p in params if p.numel() == size],
+                'update_buffer': [
+                    torch.empty(size, device='cuda', dtype=torch.bfloat16)
+                    for _ in range(self.world_size)
                 ],
             }
             for size in sizes
@@ -97,16 +89,17 @@ class Muon(torch.optim.Optimizer):
         super().__init__(param_groups, defaults)
 
     def step(self):
+
         for group in self.param_groups:
-            lr: float = group["lr"]
-            momentum: float = group["momentum"]
-            nesterov: bool = group["nesterov"]
-            zeropower_backend = zeropower_backends[group["backend"]]
-            backend_steps: int = group["backend_steps"]
-            update_buffers: "list[torch.Tensor]" = group["update_buffer"]
+
+            lr = group['lr']
+            momentum = group['momentum']
+            nesterov = group['nesterov']
+            ns_steps = group['ns_steps']
+            update_buffers = group['update_buffer']
             # generate weight updates in distributed fashion
-            params: "list[torch.Tensor]" = group["params"]
-            assert len(params) % self.num_process == 0
+            params = group['params']
+            assert len(params) % self.world_size == 0
             handle = None
             params_world = None
             def update_prev():
@@ -119,23 +112,22 @@ class Muon(torch.optim.Optimizer):
                         g_world.view_as(p_world),
                         alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                     )
-            for base_i in range(len(params))[::self.num_process]:
+            for base_i in range(len(params))[::self.world_size]:
                 p = params[base_i + self.rank]
                 g = p.grad
                 assert g is not None
-                state = self.state[p] 
-                if "momentum_buffer" not in state:
-                    state["momentum_buffer"] = torch.zeros_like(g)
-                buf: torch.Tensor = state["momentum_buffer"]
+                state = self.state[p]
+                if 'momentum_buffer' not in state:
+                    state['momentum_buffer'] = torch.zeros_like(g)
+                buf = state['momentum_buffer']
                 buf.lerp_(g, 1 - momentum)
                 g = g.lerp_(buf, momentum) if nesterov else buf
-                g = zeropower_backend(g, steps=backend_steps).flatten()
+                g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                 update_prev()
                 handle = dist.all_gather(update_buffers, g, async_op=True)
-                params_world = params[base_i : base_i + self.num_process]
+                params_world = params[base_i : base_i + self.world_size]
             update_prev()
 
-
 # -----------------------------------------------------------------------------
 # PyTorch nn.Module definitions for the GPT-2 model
 
@@ -176,44 +168,41 @@ class Rotary(torch.nn.Module):
 
 class CausalSelfAttention(nn.Module):
 
-    def __init__(self, dim, n_head):
+    def __init__(self, dim, num_heads):
         super().__init__()
-        assert dim % n_head == 0
-        self.n_head = n_head
+        assert dim % num_heads == 0
+        self.num_heads = num_heads
         self.c_q = CastedLinear(dim, dim)
         self.c_k = CastedLinear(dim, dim)
         self.c_v = CastedLinear(dim, dim)
-        # value residual lambda
-        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5])) # @Grad62304977
-        # rotary embeddings
-        self.rotary = Rotary(dim // n_head) # dim // n_head = head_dim
-        # output projection
+        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
+        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
         self.c_proj = CastedLinear(dim, dim)
         self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
 
-    def forward(self, x: torch.Tensor, vi: torch.Tensor, block_mask: BlockMask) -> torch.Tensor:
+    def forward(self, x, vi, block_mask):
         B, T = x.size(0), x.size(1) # batch size, sequence length
         assert B == 1, "Must use batch size = 1 for FlexAttention"
-        q: torch.Tensor = self.c_q(x).view(B, T, self.n_head, -1)
-        k: torch.Tensor = self.c_k(x).view(B, T, self.n_head, -1)
-        v: torch.Tensor = self.c_v(x).view(B, T, self.n_head, -1)
-        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @Grad62304977
-        q, k = norm(q), norm(k) # QK norm suggested by @Grad62304977
+        q = self.c_q(x).view(B, T, self.num_heads, -1)
+        k = self.c_k(x).view(B, T, self.num_heads, -1)
+        v = self.c_v(x).view(B, T, self.num_heads, -1)
+        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @KoszarskyB & @Grad62304977
+        q, k = norm(q), norm(k) # QK norm @Grad62304977
         q, k = self.rotary(q), self.rotary(k)
-        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
+        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, enable_gqa=True)
         y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
         y = self.c_proj(y)
         return y
 
 class MLP(nn.Module):
 
-    def __init__(self, dim: int):
+    def __init__(self, dim):
         super().__init__()
         self.c_fc   = CastedLinear(dim, 4 * dim)
         self.c_proj = CastedLinear(4 * dim, dim)
         self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x):
         x = self.c_fc(x)
         x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
         x = self.c_proj(x)
@@ -223,101 +212,132 @@ class Block(nn.Module):
 
     def __init__(self, config):
         super().__init__()
-        self.attn = CausalSelfAttention(config.n_embd, config.n_head)
-        self.mlp = MLP(config.n_embd)
+        self.attn = CausalSelfAttention(config.model_dim, config.num_heads)
+        self.mlp = MLP(config.model_dim)
         self.lambdas = nn.Parameter(torch.tensor([1., 0.]))
 
-    def forward(self, x: torch.Tensor, vi: torch.Tensor, x0: torch.Tensor, block_mask: BlockMask) -> torch.Tensor:
+    def forward(self, x, vi, x0, block_mask):
         x = self.lambdas[0] * x + self.lambdas[1] * x0
         x = x + self.attn(norm(x), vi, block_mask)
         x = x + self.mlp(norm(x))
         return x
 
+class ValueEmbedding(nn.Module):
+    def __init__(self, config: "GPTConfig"):
+        super().__init__()
+        self.__setattr__
+        self.embed = nn.ModuleList([
+            nn.Embedding(config.vocab_size, config.model_dim)
+            for _ in range(6)
+        ])
+
+    def forward(self, inputs) -> "list[torch.Tensor]":
+        ve = [emb(inputs) for emb in self.embed]
+        ve += reversed(ve)
+        return ve
+
+
 # -----------------------------------------------------------------------------
 # The main GPT-2 model
 
 @dataclass
 class GPTConfig:
     vocab_size : int = 50304
-    n_layer : int = 12
-    n_head : int = 6 # head dim 128 suggested by @Grad62304977
-    n_embd : int = 768
-    lm_head_softcap : int = 30
+    num_layers : int = 12
+    num_heads : int = 6 # head dim 128 suggested by @Grad62304977
+    model_dim : int = 768
 
 class GPT(nn.Module):
 
     def __init__(self, config: GPTConfig):
         super().__init__()
-        self.n_layer = config.n_layer
-        self.lm_head_softcap = config.lm_head_softcap
+        self.num_layers = config.num_layers
 
         # U-net design by @brendanh0gan
-        self.num_encoder_layers = config.n_layer // 2 # Half of the layers for encoder
-        self.num_decoder_layers = config.n_layer - self.num_encoder_layers # Remaining for decoder
+        self.num_encoder_layers = config.num_layers // 2 # Half of the layers for encoder
+        self.num_decoder_layers = config.num_layers - self.num_encoder_layers # Remaining for decoder
         # Add learnable skip connection weights for decoder layers
         self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
 
-        self.transformer = nn.ModuleDict(dict(
-            wte = nn.Embedding(config.vocab_size, config.n_embd),
-            # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
-            # U-net structure on token value embeddings by @leloykun
-            vte = nn.Embedding(config.vocab_size, config.n_embd*self.num_encoder_layers),
-            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
-        ))
-        self.lm_head = CastedLinear(config.n_embd, config.vocab_size)
+        self.embed = nn.Embedding(config.vocab_size, config.model_dim)
+        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)])
+        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
+        # U-net structure on token value embeddings by @leloykun
+        self.value_embeds = ValueEmbedding(config)
+        self.lm_head = CastedLinear(config.model_dim, config.vocab_size)
         self.lm_head.weight.data.zero_() # @Grad62304977
 
-    def forward(self, idx: torch.Tensor, target: torch.Tensor, sliding_window: torch.Tensor) -> torch.Tensor:
+    def forward(
+        self,
+        inputs: torch.Tensor,
+        targets: torch.Tensor,
+        sliding_window_num_blocks: torch.Tensor,
+    ):
         BLOCK_SIZE = 128
-        assert idx.ndim == 1
-        docs = (idx == 50256).cumsum(0)
-        docs_low = docs.reshape(-1, BLOCK_SIZE)[:, 0].contiguous()
-        docs_high = docs.reshape(-1, BLOCK_SIZE)[:, -1].contiguous()
-        def document_sliding_window_causal(b, h, q_idx, kv_idx):
+        assert inputs.ndim == 1
+        docs = (inputs == 50256).cumsum(0)
+        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
+        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
+
+        def document_causal(b, h, q_idx, kv_idx):
             causal_mask = q_idx >= kv_idx
             document_mask = docs[q_idx] == docs[kv_idx]
-            window_mask = q_idx - kv_idx < sliding_window
-            return causal_mask & document_mask & window_mask
+            return causal_mask & document_mask
+
+        def dense_to_ordered(dense_mask: torch.Tensor):
+            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
+            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
+            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()
 
-        S = len(idx)
-        def create_sliding_window_causal_mask(S: int, sliding_window: torch.Tensor):
-            kv_idx = block_idx = torch.arange(S // BLOCK_SIZE, dtype=torch.int32, device="cuda")
+        def create_doc_swc_block_mask(sliding_window_num_blocks: torch.Tensor):
+            kv_idx = block_idx = torch.arange(512, dtype=torch.int32, device="cuda")
             q_idx = block_idx[:, None]
-            causal_mask = q_idx >= kv_idx
-            document_mask = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
-            window_mask = q_idx - kv_idx < ((sliding_window + BLOCK_SIZE - 1) // BLOCK_SIZE)
-            dense_mask = causal_mask & document_mask & window_mask
-            dense_mask = dense_mask.to(torch.int32)
-            num_blocks = dense_mask.sum(dim=-1).to(torch.int32)
-            indices = torch.argsort(dense_mask, dim=-1, descending=True, stable=True).to(torch.int32)
-            num_blocks = num_blocks[None, None, :].contiguous()
-            indices = indices[None, None, :].contiguous()
-            return BlockMask.from_kv_blocks(num_blocks, indices, BLOCK_SIZE=BLOCK_SIZE, mask_mod=document_sliding_window_causal)
-        block_mask = create_sliding_window_causal_mask(S, sliding_window)
+            causal_bm = q_idx >= kv_idx
+            causal_full_bm = q_idx > kv_idx
+            window_bm = q_idx - kv_idx < sliding_window_num_blocks
+            window_full_bm = window_bm
+            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
+            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
+            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
+            nonzero_bm = causal_bm & window_bm & document_bm
+            full_bm  = causal_full_bm & window_full_bm & document_full_bm
+            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm ^ full_bm)
+            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
+            return BlockMask.from_kv_blocks(
+                kv_num_blocks,
+                kv_indices,
+                full_kv_num_blocks,
+                full_kv_indices,
+                BLOCK_SIZE=BLOCK_SIZE,
+                mask_mod=document_causal,
+            )
+
+        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)
 
         # forward the GPT model itself
-        x = self.transformer.wte(idx[None]) # token embeddings of shape (b, t, n_embd)
+        x = self.embed(inputs[None]) # token embeddings of shape (b, t, model_dim)
         x = norm(x) # @Grad62304977
         x0 = x
-        vi = self.transformer.vte(idx[None]).chunk(self.num_encoder_layers, dim=-1)
+        ve = self.value_embeds(inputs)
+        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
 
         # Store outputs for U-Net skip connections
         skip_connections = []
         # Encoder pass - process only the first half of the blocks
         for i in range(self.num_encoder_layers):
-            x = self.transformer.h[i](x, vi[i], x0, block_mask)
+            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
             skip_connections.append(x)
         # Decoder pass - process the remaining blocks with weighted skip connections
         for i in range(self.num_decoder_layers):
             x = x + self.skip_weights[i] * skip_connections.pop()
             # U-net structure on token value embeddings by @leloykun
-            x = self.transformer.h[self.num_encoder_layers + i](x, vi[self.num_encoder_layers-1-i], x0, block_mask)
+            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)
 
         x = norm(x)
         logits = self.lm_head(x)
-        logits = self.lm_head_softcap * torch.tanh(logits / self.lm_head_softcap) # @Grad62304977
+        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
         logits = logits.float()
-        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
+        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
         return loss
 
 # -----------------------------------------------------------------------------
@@ -331,29 +351,29 @@ def _peek_data_shard(file: Path):
     assert header[1] == 1, "unsupported version"
     return int(header[2]) # number of tokens (claimed)
 
-def _load_data_shard(file: Path, ntok: int):
-    # with file.open("rb") as f:
-    with open(file, "rb") as f:
-        tokens = torch.empty(ntok, dtype=torch.uint16, pin_memory=True)
+def _load_data_shard(path: Path, num_tokens):
+    # with path.open("rb", buffering=0) as f:
+    with open(path, "rb", buffering=0) as f:
+        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)
         f.seek(256 * 4)
         nbytes = f.readinto(tokens.numpy())
-        assert nbytes == 2 * ntok, "number of tokens read does not match header?"
+        assert nbytes == 2 * num_tokens, "number of tokens read does not match header?"
     return tokens
 
 class DistributedDataLoader:
-    def __init__(self, filename_pattern, T, process_rank, num_processes):
+    def __init__(self, filename_pattern, seq_len, process_rank, num_processes):
         self.process_rank = process_rank
         self.num_processes = num_processes
-        self.T = T
+        self.seq_len = seq_len
 
         # glob files that match the pattern
         self.files = sorted(glob.glob(filename_pattern))
         assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"
 
         # load and validate all data shards, count number of tokens in total
-        self.ntoks = [_peek_data_shard(file) for file in self.files]
-        assert min(self.ntoks) >= num_processes * T + 1
-        self.ntok_total = sum(self.ntoks)
+        self.files_num_tokens = [_peek_data_shard(file) for file in self.files]
+        assert min(self.files_num_tokens) >= num_processes * seq_len + 1
+        self.total_num_tokens = sum(self.files_num_tokens)
 
         self.reset()
 
@@ -363,21 +383,21 @@ class DistributedDataLoader:
 
     def advance(self): # advance to next data shard
         self.current_shard = (self.current_shard + 1) % len(self.files)
-        self.current_position = self.process_rank * self.T
-        self.tokens = _load_data_shard(self.files[self.current_shard], self.ntoks[self.current_shard])
+        self.current_position = self.process_rank * self.seq_len
+        self.tokens = _load_data_shard(self.files[self.current_shard], self.files_num_tokens[self.current_shard])
 
     def next_batch(self):
-        batch_size = self.T * self.num_processes
-        buf = self.tokens[self.current_position:self.current_position+self.T+1]
+        batch_size = self.seq_len * self.num_processes
+        buf = self.tokens[self.current_position:self.current_position+self.seq_len+1]
         # host side async is sufficient;
         # no performance improvement was observed when introducing a separate stream.
-        x = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
-        y = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # targets
+        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
+        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # targets
         # advance current position and load next shard if necessary
         self.current_position += batch_size
         if self.current_position + batch_size + 1 >= len(self.tokens):
             self.advance()
-        return x, y
+        return inputs, targets
 
 # -----------------------------------------------------------------------------
 # int main
@@ -401,79 +421,80 @@ class Hyperparameters:
 args = Hyperparameters()
 
 # set up DDP (distributed data parallel). torchrun sets this env variable
-assert torch.cuda.is_available()
-dist.init_process_group(backend='nccl')
 ddp_rank = int(os.environ['RANK'])
 ddp_local_rank = int(os.environ['LOCAL_RANK'])
 ddp_world_size = int(os.environ['WORLD_SIZE'])
-device = f'cuda:{ddp_local_rank}'
+assert torch.cuda.is_available()
+device = torch.device(f"cuda:{ddp_local_rank}")
 torch.cuda.set_device(device)
 print(f"using device: {device}")
+dist.init_process_group(backend='nccl', device_id=device)
+dist.barrier()
 master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.
 
 # begin logging
 logfile = None
 if master_process:
-    run_id = str(uuid.uuid4())
-    logdir = 'logs/%s/' % run_id
-    os.makedirs(logdir, exist_ok=True)
-    logfile = 'logs/%s.txt' % run_id
+    run_id = uuid.uuid4()
+    os.makedirs("logs", exist_ok=True)
+    logdir = Path("logs") / f"{run_id}"
+    logdir.mkdir(exist_ok=True)
+    logfile = Path("logs") / f"{run_id}.txt"
+    print(logfile.stem)
     # create the log file
-    with open(logfile, "w") as f:
+    with logfile.open("w") as f:
         # begin the log by printing this file (the Python code)
-        f.write(code)
-        f.write('='*100 + '\n')
+        print(code, file=f)
+        print("=" * 100, file=f)
 def print0(s, logonly=False):
     if master_process:
-        with open(logfile, "a") as f:
+        with logfile.open("a") as f:
             if not logonly:
                 print(s)
-            f.write(s+'\n')
+            print(s, file=f)
 # log information about the hardware/software environment this is running on
 # and print the full `nvidia-smi` to file
+print0(f"Running python {sys.version}")
 print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
 import subprocess
 result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
 print0(f'{result.stdout}', logonly=True)
 print0('='*100, logonly=True)
 
-# convenience variables
-T = args.sequence_length
 # calculate the number of steps to take in the val loop.
-assert args.val_tokens % (T * ddp_world_size) == 0
-val_steps = args.val_tokens // (T * ddp_world_size)
+assert args.val_tokens % (args.sequence_length * ddp_world_size) == 0
+val_steps = args.val_tokens // (args.sequence_length * ddp_world_size)
 # calculate the steps of gradient accumulation required to attain the desired global batch size.
 assert args.batch_size % (ddp_world_size) == 0
 train_accumulation_steps = args.batch_size // ddp_world_size
-assert train_accumulation_steps == 1
 
 # load tokens
-train_loader = DistributedDataLoader(args.input_bin, T, ddp_rank, ddp_world_size)
-val_loader = DistributedDataLoader(args.input_val_bin, T, ddp_rank, ddp_world_size)
-print0(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
-print0(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
+train_loader = DistributedDataLoader(args.input_bin, args.sequence_length, ddp_rank, ddp_world_size)
+val_loader = DistributedDataLoader(args.input_val_bin, args.sequence_length, ddp_rank, ddp_world_size)
+print0(f"Training DataLoader: total number of tokens: {train_loader.total_num_tokens} across {len(train_loader.files)} files")
+print0(f"Validation DataLoader: total number of tokens: {val_loader.total_num_tokens} across {len(val_loader.files)} files")
 print0('='*100, logonly=True)
-x, y = train_loader.next_batch()
+inputs_train, targets_train = train_loader.next_batch()
 
 # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
 # this originates from Karpathy's experiments.
 num_vocab = 50304
-model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
+model = GPT(GPTConfig(vocab_size=num_vocab, num_layers=12, num_heads=6, model_dim=768))
 model = model.cuda().bfloat16()
 for m in model.modules():
     if isinstance(m, CastedLinear):
         m.float()
-if hasattr(config, "coordinate_descent_tuning"):
-    config.coordinate_descent_tuning = True # suggested by @Chillee
+config.coordinate_descent_tuning = True # suggested by @Chillee
 model = torch.compile(model)
 # here we wrap model into DDP container
-model = DDP(model, device_ids=[ddp_local_rank])
+model = DDP(model, device_ids=[ddp_local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)
 raw_model = model.module # always contains the "raw" unwrapped model
 
 # init the optimizer(s)
-optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight, raw_model.transformer.vte.weight], lr=0.6, betas=(0.8, 0.95), fused=True)
+embed_params = [*raw_model.embed.parameters(), *raw_model.value_embeds.parameters()]
+optimizer1 = torch.optim.Adam(embed_params, lr=0.6, betas=(0.8, 0.95), fused=True)
 optimizer2 = torch.optim.Adam([raw_model.lm_head.weight], lr=0.008, betas=(0.8, 0.95), fused=True)
-params = list(raw_model.transformer.h.parameters())
+params = list(raw_model.blocks.parameters())
 matrix_params = [p for p in params if p.ndim == 2]
 scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
 optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
@@ -494,8 +515,8 @@ def get_lr(it):
         return decay_ratio
 schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
 
-sliding_window_size = torch.tensor(64, dtype=torch.int32, device="cuda")
-sw_size_prev = 64
+sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device="cuda")
+sw_num_blocks_prev = 1
 # Start training loop
 training_time_ms = 0
 # start the clock
@@ -512,11 +533,12 @@ for step in range(args.num_iterations + 1):
         t0 = time.perf_counter()
     timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val
 
-    # Set the sliding window size for the current step, in chunks of 64. By @fernbear.bsky.social
-    sw_size =  64 * int((64 + (1792 - 64) * step / args.num_iterations) // 64)
-    if sw_size != sw_size_prev:
-        sliding_window_size.copy_(sw_size, non_blocking=True)
-        sw_size_prev = sw_size
+    # Linearly increase the sliding window size over training in chunks of 64 from 64 -> 1792. By @fernbear.bsky.social
+    frac_done = step / args.num_iterations # training progress
+    sw_num_blocks = int(((1 - frac_done) * 64 + frac_done * 1792 + 64) // 128)
+    if sw_num_blocks != sw_num_blocks_prev:
+        sliding_window_num_blocks.copy_(sw_num_blocks, non_blocking=True)
+        sw_num_blocks_prev = sw_num_blocks
 
     # once in a while evaluate the validation dataset
     if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
@@ -529,8 +551,8 @@ for step in range(args.num_iterations + 1):
         val_loss = 0.0
         for _ in range(val_steps):
             with torch.no_grad():
-                x_val, y_val = val_loader.next_batch()
-                val_loss += model(x_val, y_val, sliding_window=sliding_window_size)
+                inputs_val, targets_val = val_loader.next_batch()
+                val_loss += model(inputs_val, targets_val, sliding_window_num_blocks)
         dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
         val_loss /= val_steps
         # log val loss to console and to logfile
@@ -545,7 +567,7 @@ for step in range(args.num_iterations + 1):
         training_time_ms += 1000 * (time.perf_counter() - t0)
         # save the state of the training process
         log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
-        # torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
+        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
         # start the clock again
         torch.cuda.synchronize()
         t0 = time.perf_counter()
@@ -559,11 +581,17 @@ for step in range(args.num_iterations + 1):
 
     # --------------- TRAINING SECTION BEGIN -----------------
     model.train()
-    loss = model(x, y, sliding_window=sliding_window_size)
-    loss.backward()
-    del loss
-    # advance the dataset for the next batch
-    x, y = train_loader.next_batch()
+    for i in range(1, train_accumulation_steps + 1):
+        with contextlib.ExitStack() as stack:
+            if i < train_accumulation_steps: # there's no need to sync gradients every accumulation step
+                stack.enter_context(model.no_sync())
+            if step >= 5:
+                stack.enter_context(torch.compiler.set_stance(skip_guard_eval_unsafe=True))
+            model(inputs_train, targets_train, sliding_window_num_blocks).backward()
+            inputs_train, targets_train = train_loader.next_batch()
+    if train_accumulation_steps != 1:
+        for p in model.parameters():
+            p.grad /= train_accumulation_steps
     # momentum warmup for Muon
     frac = min(step/300, 1)
     for group in optimizer3.param_groups:
@@ -579,9 +607,8 @@ for step in range(args.num_iterations + 1):
     approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
     print0(f"step:{step+1}/{args.num_iterations} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
 
-if master_process:
-    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
+print0(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
 
 # -------------------------------------------------------------------------
 # clean up nice
-dist.destroy_process_group()
\ No newline at end of file
+dist.destroy_process_group()
