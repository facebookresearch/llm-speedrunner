diff --git a/temp_current.py b/temp_next.py
index f27ff22..71ede92 100644
--- a/temp_current.py
+++ b/temp_next.py
@@ -19,6 +19,34 @@ with open(sys.argv[0]) as f:
 # -----------------------------------------------------------------------------
 # PyTorch nn.Module definitions for the GPT-2 model
 
+class Rotary(torch.nn.Module):
+    def __init__(self, dim, base=10000):
+        super().__init__()
+        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
+        self.register_buffer("inv_freq", inv_freq)
+        self.seq_len_cached = None
+        self.cos_cached = None
+        self.sin_cached = None
+
+    def forward(self, x):
+        seq_len = x.shape[1]
+        if seq_len != self.seq_len_cached:
+            self.seq_len_cached = seq_len
+            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
+            freqs = torch.outer(t, self.inv_freq).to(x.device)
+            self.cos_cached = freqs.cos()
+            self.sin_cached = freqs.sin()
+        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
+
+def apply_rotary_emb(x, cos, sin):
+    assert x.ndim == 4 # multihead attention
+    d = x.shape[3]//2
+    x1 = x[..., :d]
+    x2 = x[..., d:]
+    y1 = x1 * cos + x2 * sin
+    y2 = x1 * (-sin) + x2 * cos
+    return torch.cat([y1, y2], 3)
+
 def rmsnorm(x0, eps=1e-6):
     x = x0.float()
     x = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + eps)
@@ -28,28 +56,31 @@ class CausalSelfAttention(nn.Module):
 
     def __init__(self, config):
         super().__init__()
-        assert config.n_embd % config.n_head == 0
-        # key, query, value projections for all heads, but in a batch
-        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)
-        # output projection
-        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)
-        # regularization
         self.n_head = config.n_head
         self.n_embd = config.n_embd
+        self.head_dim = self.n_embd // self.n_head
+        assert self.n_embd % self.n_head == 0
+        # key, query, value projections for all heads, but in a batch
+        self.c_attn = nn.Linear(self.n_embd, 3 * self.n_embd, bias=False)
+        # output projection
+        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
+        self.rotary = Rotary(self.head_dim)
 
     def forward(self, x):
         B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
         # calculate query, key, values for all heads in batch and move head forward to be the batch dim
         qkv = self.c_attn(x)
         q, k, v = qkv.split(self.n_embd, dim=2)
-        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
-        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
-        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
-        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)
+        k = k.view(B, T, self.n_head, self.head_dim)
+        q = q.view(B, T, self.n_head, self.head_dim)
+        v = v.view(B, T, self.n_head, self.head_dim)
+        cos, sin = self.rotary(q)
+        q = apply_rotary_emb(q, cos, sin)
+        k = apply_rotary_emb(k, cos, sin)
+        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
         y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side
         # output projection
         y = self.c_proj(y)
-        y = y / math.sqrt(24)
         return y
 
 class MLP(nn.Module):
@@ -71,9 +102,10 @@ class Block(nn.Module):
         super().__init__()
         self.attn = CausalSelfAttention(config)
         self.mlp = MLP(config)
+        self.attn_scale = (1 / math.sqrt(2 * config.n_layer))
 
     def forward(self, x):
-        x = x + self.attn(rmsnorm(x))
+        x = x + self.attn_scale * self.attn(rmsnorm(x))
         x = x + self.mlp(rmsnorm(x))
         return x
 
@@ -96,18 +128,10 @@ class GPT(nn.Module):
 
         self.transformer = nn.ModuleDict(dict(
             wte = nn.Embedding(config.vocab_size, config.n_embd),
-            wpe = nn.Embedding(config.block_size, config.n_embd),
             h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
         ))
         self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
-        self.lm_head.LLMC_SKIP_INIT = 1 # don't init this one, we will tie weights
         self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying
-        self.apply(self._init_weights)
-
-    def _init_weights(self, module):
-        # initialize the position embedding at std=0.02 to match the scale of the token embedding.
-        if isinstance(module, nn.Embedding) and not hasattr(module, 'LLMC_SKIP_INIT'):
-            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
 
     def forward(self, idx, targets=None, return_logits=True):
         b, t = idx.size()
@@ -115,9 +139,7 @@ class GPT(nn.Module):
         pos = torch.arange(0, t, dtype=torch.long, device=idx.device) # shape (t)
 
         # forward the GPT model itself
-        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
-        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)
-        x = tok_emb + pos_emb
+        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
 
         for block in self.transformer.h:
             x = block(x)
@@ -229,34 +251,31 @@ def print0(*args, **kwargs):
 
 @dataclass
 class Hyperparameters:
-    # data
-    # train_files = os.environ["NANOGPT_TRAIN_FILES"] # input .bin to train on
+    # data hyperparams
     input_bin = os.environ["NANOGPT_TRAIN_FILES"]
-    # val_files = os.environ["NANOGPT_VAL_FILES"] # input .bin to eval validation loss on
     input_val_bin = os.environ["NANOGPT_VAL_FILES"]
     val_tokens = int(os.environ["NANOGPT_VAL_TOKENS"]) # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
+
     model = "d12"
     # optimization
-    batch_size = 32 # batch size in tokens
+    batch_size = 64 # batch size in tokens
     sequence_length = 1024 # sequence length
-    total_batch_size = 262144# total desired batch size, in units of #tokens
-    num_iterations = 24576# number of iterations to run
-
-    learning_rate = 0.0015
-    warmup_iters = 256
+    total_batch_size = 524288
+    num_iterations = 9536 # number of iterations to run
+    val_loss_every = 128
     weight_decay = 0.1
-    grad_clip = 1.0
-    # evaluation
-    val_loss_every = 128 # every how many steps to evaluate val loss? 0 for only at the end
-    val_max_steps = 20 # how many batches of val to average?
+    learning_rate = 0.0018
+    warmup_iters = 256
+    warmdown_iters = 2000
+    save_every = 0
 
-    output_dir = "pylog124m"
+    val_max_steps = 20
 
+    output_dir = "pylog124m"
 
 if __name__ == "__main__":
     import time
     import argparse
-    import tiktoken
     print0(f"Running pytorch {torch.version.__version__}")
 
     args = Hyperparameters()
@@ -285,9 +304,6 @@ if __name__ == "__main__":
     # set up a context manager following the desired dtype and device
     ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)
 
-    # init (and write) the tokenizer
-    enc = tiktoken.get_encoding("gpt2")
-
     # init the model from scratch
     model_config = {
         "d12": GPTConfig(block_size=1024, vocab_size=50257, n_layer=12, n_head=12, n_embd=768),
@@ -296,11 +312,11 @@ if __name__ == "__main__":
         "d48": GPTConfig(block_size=1024, vocab_size=50257, n_layer=48, n_head=25, n_embd=1600),
     }[args.model]
     model = GPT(model_config)
-    model = model.train()#.cuda()
+    model = model.train().cuda()
     if hasattr(config, "coordinate_descent_tuning"):
         config.coordinate_descent_tuning = True # suggested by @Chillee
     print0("compiling the model...")
-    model = torch.compile(model).cuda()
+    model = torch.compile(model)
 
     # load tokens
     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
@@ -318,16 +334,19 @@ if __name__ == "__main__":
                                                learning_rate=args.learning_rate, betas=(0.9, 0.95),
                                                device_type=device)
 
-    # learning rate decay scheduler (cosine with warmup)
+    # learning rate decay scheduler (linear warmup and warmdown)
     def get_lr(it):
         assert it <= args.num_iterations
         # 1) linear warmup for warmup_iters steps
         if it < args.warmup_iters:
             return args.learning_rate * (it+1) / args.warmup_iters
-        # 2) linear decay down to min learning rate
-        decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)
-        assert 0 <= decay_ratio <= 1
-        return (0.1 + (1 - decay_ratio)) / (0.1 + 1) * args.learning_rate
+        # 2) constant lr for a while
+        elif it < args.num_iterations - args.warmdown_iters:
+            return args.learning_rate
+        # 3) linear warmdown
+        else:
+            decay_ratio = (args.num_iterations - it) / args.warmdown_iters
+            return args.learning_rate * decay_ratio
 
     run_id = str(uuid.uuid4())
 
@@ -341,7 +360,6 @@ if __name__ == "__main__":
             pass
 
     timings = []
-    norm = -1.0   # dummy value to print in inference-only mode
     for step in range(args.num_iterations + 1):
         t0 = time.time()
         last_step = (step == args.num_iterations)
@@ -381,7 +399,8 @@ if __name__ == "__main__":
         x, y = train_loader.next_batch()
         # backward pass
         loss.backward()
-        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
+        for p in model.parameters():
+            p.grad = p.grad / (p.grad.norm() + 1e-6)
         # determine and set the learning rate for this iteration
         lr = get_lr(step)
         for param_group in optimizer.param_groups:
@@ -398,7 +417,7 @@ if __name__ == "__main__":
         # the 0th iteration is often an outlier (much slower) => skip logging it
         tokens_per_second = ddp_world_size * B * T / (t1-t0)
         lossf = loss.item() # keep track of the mean loss
-        print0(f"step {step+1:4d}/{args.num_iterations} | train loss {lossf:.6f} | norm {norm:.4f} | lr {lr:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)")
+        print0(f"step {step+1:4d}/{args.num_iterations} | train loss {lossf:.6f} | lr {lr:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)")
         # log to logile
         if master_process and logfile is not None:
             with open(logfile, "a") as f:
@@ -408,6 +427,11 @@ if __name__ == "__main__":
         if step > 0 and step > args.num_iterations - 20:
             timings.append(t1-t0)
 
+        if master_process and (args.save_every > 0 and step % args.save_every == 0):
+            log = dict(model=raw_model.state_dict(), code=code, args=args.__dict__)
+            os.makedirs('logs/%s' % run_id, exist_ok=True)
+            torch.save(log, 'logs/%s/model_step%06d.pt' % (run_id, step))
+
     # print the average of the last 20 timings, to get something smooth-ish
     timings = timings[-20:]
     print0(f"final {len(timings)} iters avg: {np.mean(timings)*1000:.3f}ms")
@@ -417,8 +441,8 @@ if __name__ == "__main__":
 
     if master_process:
         log = dict(model=raw_model.state_dict(), code=code, args=args.__dict__)
-        os.makedirs('logs', exist_ok=True)
-        torch.save(log, 'logs/%s.pt' % run_id)
+        os.makedirs('logs/%s' % run_id, exist_ok=True)
+        torch.save(log, 'logs/%s/final.pt' % run_id)
 
     # -------------------------------------------------------------------------
     # clean up nice
