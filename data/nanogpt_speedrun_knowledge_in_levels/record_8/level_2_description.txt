
1. **Architectural Shortcuts (Value and Embedding Skip Connections)**
   - **Implementation**: Added learnable blending between current values and first block outputs (`v1`) in attention layers. Introduced residual connections to initial embeddings (`x0`) using parameterized weights (`self.lambdas`).
   - **Benefit**: Preserves critical early-layer information through the network, combats vanishing gradients, and improves feature reuse. Learnable parameters let the model adapt blending ratios.
   - **Performance Impact**: Accounts for ~43% of speedup by reducing redundant computation and improving gradient flow.
   - **Technical Challenge**: Required careful parameter initialization and dimension matching for skip connections without introducing instability.

2. **Momentum Warmup for Muon Optimizer**
   - **Implementation**: Linear momentum increase from 0.85 → 0.95 over first 500 steps (`optimizer3.param_groups[0]['momentum']` adjustment).
   - **Benefit**: Stabilizes early training with conservative updates, then leverages full momentum for faster convergence later.
   - **Performance Impact**: Prevents early optimization instability while maintaining final convergence quality.
   - **Technical Challenge**: Required modifying optimizer state handling and ensuring compatibility with distributed training.

3. **Tanh Logit Capping**
   - **Implementation**: Added `30 * torch.tanh(logits/30)` before loss calculation.
   - **Benefit**: Prevents logit explosion (common in final layers) while maintaining relative ordering. Inspired by Gemma 2's stability improvements.
   - **Performance Impact**: Enables stable training with higher learning rates for output layers.
   - **Technical Challenge**: Required empirical tuning of the 30× scaling factor to balance stability and expressiveness.

4. **Parameter-Type-Specific Optimization**
   - **Implementation**: Separated parameters into:
     - Matrix params (2D): Optimized with Muon
     - Scalar params (λ weights): Optimized with Adam
   - **Benefit**: Properly handles non-2D parameters that Muon can't optimize, while maintaining Muon's benefits for weight matrices.
   - **Performance Impact**: Ensures all parameters receive appropriate optimization attention.
   - **Technical Challenge**: Required parameter filtering logic and multi-optimizer coordination.

**System-Level Improvements**
- Reduced total iterations from 4578 → 3200 through faster convergence
- Adjusted warmdown schedule (1308 → 914 steps) to match new training dynamics
- Modified model compilation order (`torch.compile` after CUDA placement) for better inductor performance

**Cumulative Impact**
These changes synergistically improve:
1. **Information Flow**: Skip connections reduce signal degradation in deep layers
2. **Optimization Stability**: Momentum warmup + logit capping prevent early divergence
3. **Parameter Efficiency**: Learnable blending weights add minimal parameters (<0.1% increase) for substantial performance gains
4. **Training Speed**: 32% faster time-to-accuracy through improved convergence

The combination of architectural improvements and optimization tweaks enabled a new speed record (3.28 validation loss in 8.2 minutes vs previous 10.8 minutes) while maintaining numerical stability on 8×H100 GPUs.